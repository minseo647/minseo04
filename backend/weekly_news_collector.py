"""
Weekly News Collector
1ì£¼ì¼ê°„ì˜ ìµœì‹  ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•˜ëŠ” ê²½ëŸ‰í™”ëœ ìˆ˜ì§‘ê¸°
"""

import os
import json
import time
import logging
from typing import List, Dict, Optional, Set
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

import requests
import feedparser
from bs4 import BeautifulSoup

# Database import
from database import db

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration for 1-week collection (ë” ì œí•œì ìœ¼ë¡œ)
MAX_RESULTS = int(os.getenv("WEEKLY_MAX_RESULTS", "3"))  # ê° ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 3ê°œë¡œ ì¶•ì†Œ
MAX_TOTAL_PER_SOURCE = int(os.getenv("WEEKLY_MAX_TOTAL_PER_SOURCE", "5"))  # ì†ŒìŠ¤ë‹¹ ì´ 5ê°œë¡œ ì¶•ì†Œ
COLLECT_DAYS = int(os.getenv("WEEKLY_COLLECT_DAYS", "7"))  # 1ì£¼ì¼ ìœ ì§€

CONNECT_TIMEOUT = float(os.getenv("CONNECT_TIMEOUT", "10.0"))
READ_TIMEOUT = float(os.getenv("READ_TIMEOUT", "15.0"))
PARALLEL_MAX_WORKERS = int(os.getenv("PARALLEL_MAX_WORKERS", "4"))  # ì¤„ì—¬ì„œ ë¶€í•˜ ê°ì†Œ

# í•µì‹¬ RSS í”¼ë“œë§Œ ì„ ë³„ (ê¸°ì¡´ 30ê°œì—ì„œ 15ê°œë¡œ ì¶•ì†Œ)
WEEKLY_FEEDS = [
    # Korean Tech News (í•µì‹¬ ì†ŒìŠ¤ë§Œ)
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìì‹ ë¬¸_ì†ë³´", "category": "IT", "lang": "ko"},
    {"feed_url": "https://zdnet.co.kr/news/news_xml.asp", "source": "ZDNet Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.itworld.co.kr/rss/all.xml", "source": "ITWorld Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter", "category": "IT", "lang": "ko"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum", "category": "Startup", "lang": "ko"},
    {"feed_url": "https://www.boannews.com/media/news_rss.xml", "source": "ë³´ì•ˆë‰´ìŠ¤", "category": "Security", "lang": "ko"},
    {"feed_url": "https://it.chosun.com/rss.xml", "source": "ITì¡°ì„ ", "category": "IT", "lang": "ko"},
    
    # Global Tech News (í•µì‹¬ ì†ŒìŠ¤ë§Œ)
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss", "source": "WIRED", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.engadget.com/rss.xml", "source": "Engadget", "category": "Tech", "lang": "en"},
    {"feed_url": "https://venturebeat.com/category/ai/feed/", "source": "VentureBeat AI", "category": "AI", "lang": "en"},
    {"feed_url": "https://arstechnica.com/feed/", "source": "Ars Technica", "category": "Tech", "lang": "en"},
    {"feed_url": "https://spectrum.ieee.org/rss/fulltext", "source": "IEEE Spectrum", "category": "Engineering", "lang": "en"},
]

# HTTP Session
HEADERS = {"User-Agent": "Mozilla/5.0 (WeeklyNewsBot/1.0)"}
session = requests.Session()
session.headers.update(HEADERS)

class WeeklyNewsCollector:
    def __init__(self):
        self.session = session
        self.cutoff_date = datetime.now() - timedelta(days=COLLECT_DAYS)
        
    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        """
        ë‹¨ì¼ RSS í”¼ë“œì—ì„œ 1ì£¼ì¼ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘
        """
        feed_url = feed_config["feed_url"]
        source = feed_config["source"]
        
        try:
            logger.info(f"ğŸ“¡ ì£¼ê°„ ìˆ˜ì§‘ ì‹œì‘: {source}")
            
            # RSS í”¼ë“œ íŒŒì‹±
            response = self.session.get(
                feed_url, 
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )
            
            if response.status_code != 200:
                logger.warning(f"âŒ {source}: HTTP {response.status_code}")
                return []
                
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"âŒ {source}: í”¼ë“œ ì—”íŠ¸ë¦¬ ì—†ìŒ")
                return []
            
            articles = []
            processed_links = set()
            
            for entry in feed.entries[:MAX_RESULTS]:  # ê° ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 3ê°œë§Œ
                try:
                    title = getattr(entry, 'title', '').strip()
                    link = getattr(entry, 'link', '').strip()
                    
                    if not title or not link or link in processed_links:
                        continue
                        
                    processed_links.add(link)
                    
                    # ë‚ ì§œ íŒŒì‹± ë° 1ì£¼ì¼ í•„í„°
                    published_str = getattr(entry, 'published', '')
                    if published_str:
                        try:
                            published = datetime(*entry.published_parsed[:6])
                            if published < self.cutoff_date:
                                continue  # 1ì£¼ì¼ ì´ì „ ê¸°ì‚¬ëŠ” ê±´ë„ˆëœ€
                        except:
                            published = datetime.now()  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„
                    else:
                        published = datetime.now()
                    
                    # ìš”ì•½ ìƒì„±
                    summary = getattr(entry, 'summary', '')
                    if summary:
                        # HTML íƒœê·¸ ì œê±°
                        soup = BeautifulSoup(summary, 'html.parser')
                        summary = soup.get_text()[:500]  # 500ì ì œí•œ
                    
                    article = {
                        'title': title,
                        'link': link,
                        'published': published.isoformat(),
                        'source': source,
                        'summary': summary,
                        'category': feed_config.get('category', ''),
                        'language': feed_config.get('lang', ''),
                        'collected_at': datetime.now().isoformat()
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜ ({source}): {e}")
                    continue
            
            logger.info(f"âœ… {source}: {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ {source} ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def save_articles_to_db(self, articles: List[Dict]) -> Dict[str, int]:
        """
        ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ ì œê±°)
        """
        if not articles:
            return {'inserted': 0, 'skipped': 0}
        
        stats = {'inserted': 0, 'skipped': 0, 'updated': 0}
        
        try:
            for article in articles:
                try:
                    # ì¤‘ë³µ ì²´í¬ (link ê¸°ì¤€)
                    existing = db.execute_query(
                        "SELECT id FROM articles WHERE link = %s" 
                        if db.db_type == "postgresql" 
                        else "SELECT id FROM articles WHERE link = ?",
                        [article['link']]
                    )
                    
                    if existing:
                        stats['skipped'] += 1
                        continue
                    
                    # ìƒˆ ê¸°ì‚¬ ì‚½ì…
                    if db.db_type == "postgresql":
                        query = """
                            INSERT INTO articles (title, link, published, source, summary, created_at)
                            VALUES (%s, %s, %s, %s, %s, NOW())
                        """
                    else:
                        query = """
                            INSERT INTO articles (title, link, published, source, summary, created_at)
                            VALUES (?, ?, ?, ?, ?, datetime('now'))
                        """
                    
                    db.execute_query(query, [
                        article['title'],
                        article['link'], 
                        article['published'],
                        article['source'],
                        article['summary']
                    ])
                    
                    stats['inserted'] += 1
                    
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {e}")
                    stats['skipped'] += 1
            
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ - ì‹ ê·œ: {stats['inserted']}, ì¤‘ë³µ: {stats['skipped']}")
            return stats
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return stats
    
    async def collect_weekly_news(self) -> Dict:
        """
        1ì£¼ì¼ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë¹„ë™ê¸°)
        """
        start_time = time.time()
        all_articles = []
        successful_feeds = 0
        failed_feeds = 0
        
        logger.info(f"ğŸš€ 1ì£¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ ({len(WEEKLY_FEEDS)}ê°œ ì†ŒìŠ¤)")
        
        # ë³‘ë ¬ ìˆ˜ì§‘
        with ThreadPoolExecutor(max_workers=PARALLEL_MAX_WORKERS) as executor:
            futures = {
                executor.submit(self.collect_from_feed, feed): feed 
                for feed in WEEKLY_FEEDS
            }
            
            for future in as_completed(futures):
                feed = futures[future]
                try:
                    articles = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                    if articles:
                        all_articles.extend(articles)
                        successful_feeds += 1
                    else:
                        failed_feeds += 1
                except Exception as e:
                    logger.error(f"âŒ {feed['source']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    failed_feeds += 1
        
        # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
        unique_articles = []
        seen_links = set()
        
        for article in all_articles:
            if article['link'] not in seen_links:
                seen_links.add(article['link'])
                unique_articles.append(article)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        save_stats = self.save_articles_to_db(unique_articles)
        
        duration = time.time() - start_time
        
        result = {
            'status': 'success',
            'duration': f"{duration:.2f}ì´ˆ",
            'stats': {
                'total_processed': len(all_articles),
                'total_unique': len(unique_articles),
                'total_inserted': save_stats['inserted'],
                'total_updated': save_stats.get('updated', 0),
                'total_skipped': save_stats['skipped']
            },
            'successful_feeds': successful_feeds,
            'failed_feeds': failed_feeds,
            'total_feeds': len(WEEKLY_FEEDS),
            'collection_period': f"{COLLECT_DAYS}ì¼",
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"ğŸ‰ 1ì£¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ! ì‹ ê·œ {save_stats['inserted']}ê°œ ê¸°ì‚¬")
        return result

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
weekly_collector = WeeklyNewsCollector()

async def collect_weekly_news_async() -> Dict:
    """
    1ì£¼ì¼ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤)
    """
    return await weekly_collector.collect_weekly_news()