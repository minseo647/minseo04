"""
JSON Data Loader for Historical News Data
1ë…„ì¹˜ ë¯¸ë¦¬ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path

# Database import
try:
    from database import db
except (ImportError, ModuleNotFoundError):
    db = None
    logging.warning("Database module not available. DB operations will be skipped.")


logger = logging.getLogger(__name__)

class JSONDataLoader:
    def __init__(self, json_file_path: str = None):
        """
        Initialize JSON data loader
        
        Args:
            json_file_path: JSON íŒŒì¼ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        """
        if json_file_path is None:
            # ê¸°ë³¸ ê²½ë¡œ: backend/../news_data.json
            current_dir = Path(__file__).parent
            self.json_file_path = current_dir.parent / "news_data.json"
        else:
            self.json_file_path = Path(json_file_path)
            
        self.articles_data = []
        self.loaded = False
        
    def load_data(self) -> bool:
        """
        JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë”©
        
        Returns:
            bool: ë¡œë”© ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not self.json_file_path.exists():
                logger.warning(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.json_file_path}")
                return False
                
            logger.info(f"JSON ë°ì´í„° ë¡œë”© ì¤‘: {self.json_file_path}")
            
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                # Handle potential empty file
                content = f.read()
                if not content:
                    logger.warning(f"JSON íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {self.json_file_path}")
                    self.articles_data = []
                    self.loaded = True
                    return True
                data = json.loads(content)
                
            # ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ì •ê·œí™”
            if isinstance(data, list):
                self.articles_data = data
            elif isinstance(data, dict) and 'articles' in data:
                self.articles_data = data['articles']
            else:
                logger.error("JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
                
            logger.info(f"âœ… {len(self.articles_data)}ê°œì˜ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¡œë”©í–ˆìŠµë‹ˆë‹¤.")
            self.loaded = True
            return True
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e} in {self.json_file_path}")
            return False
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def save_articles_to_db(self) -> Dict[str, int]:
        """
        Loaded articles to the database (checking for duplicates).
        """
        if not self.loaded:
            if not self.load_data():
                return {'inserted': 0, 'skipped': 0, 'failed': 0}
        
        if db is None:
            logger.error("Database module not available. Cannot save to DB.")
            return {'inserted': 0, 'skipped': 0, 'failed': len(self.articles_data)}

        stats = {'inserted': 0, 'skipped': 0, 'failed': 0}
        
        logger.info(f"ğŸ’¾ Saving {len(self.articles_data)} articles from {self.json_file_path.name} to database...")

        for article in self.articles_data:
            try:
                link = article.get('link')
                if not link:
                    stats['failed'] += 1
                    continue

                # Check for duplicates (by link)
                existing = db.execute_query(
                    "SELECT id FROM articles WHERE link = %s" if db.db_type == "postgresql" else "SELECT id FROM articles WHERE link = ?",
                    [link]
                )
                
                if existing:
                    stats['skipped'] += 1
                    continue
                
                # Insert new article
                title = article.get('title', 'No Title')
                published = article.get('published', datetime.now().isoformat())
                source = article.get('source', 'Unknown Source')
                summary = article.get('summary', '')
                raw_text = article.get('raw_text', article.get('content', ''))
                keywords = json.dumps(article.get('keywords', []), ensure_ascii=False)

                if db.db_type == "postgresql":
                    query = """
                        INSERT INTO articles (title, link, published, source, summary, raw_text, keywords, created_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                    """
                else: # sqlite
                    query = """
                        INSERT INTO articles (title, link, published, source, summary, raw_text, keywords, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'))
                    """
                
                db.execute_query(query, [
                    title,
                    link, 
                    published,
                    source,
                    summary,
                    raw_text,
                    keywords
                ])
                
                stats['inserted'] += 1
                
            except Exception as e:
                logger.debug(f"Article saving error: {e} - Article: {article.get('title')}")
                stats['failed'] += 1
        
        logger.info(f"âœ… Finished saving from {self.json_file_path.name} - Inserted: {stats['inserted']}, Skipped: {stats['skipped']}, Failed: {stats['failed']}")
        return stats

    def get_articles(self, limit: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """
        ê¸°ì‚¬ ë°ì´í„° ë°˜í™˜
        
        Args:
            limit: ë°˜í™˜í•  ê¸°ì‚¬ ìˆ˜ ì œí•œ
            offset: ì‹œì‘ ì¸ë±ìŠ¤
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not self.loaded:
            if not self.load_data():
                return []
        
        start_idx = offset
        end_idx = None if limit is None else offset + limit
        
        return self.articles_data[start_idx:end_idx]
    
    def get_articles_by_date_range(self, days_back: int = 365) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ê¸°ê°„ì˜ ê¸°ì‚¬ ë°˜í™˜
        
        Args:
            days_back: í˜„ì¬ë¡œë¶€í„° ë©°ì¹  ì „ê¹Œì§€ì˜ ê¸°ì‚¬
            
        Returns:
            List[Dict]: í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.loaded:
            if not self.load_data():
                return []
                
        cutoff_date = datetime.now() - timedelta(days=days_back)
        filtered_articles = []
        
        for article in self.articles_data:
            try:
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
                published = article.get('published', '')
                if published:
                    # ISO í˜•ì‹ ì‹œë„
                    try:
                        article_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    except:
                        # ë‹¤ë¥¸ í˜•ì‹ë“¤ ì‹œë„
                        from dateutil import parser
                        article_date = parser.parse(published)
                    
                    if article_date >= cutoff_date:
                        filtered_articles.append(article)
            except Exception as e:
                logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {article.get('title', 'Unknown')}, {e}")
                continue
                
        logger.info(f"ğŸ“… ìµœê·¼ {days_back}ì¼ê°„ ê¸°ì‚¬ {len(filtered_articles)}ê°œ ë°˜í™˜")
        return filtered_articles
    
    def get_sources(self) -> List[str]:
        """
        ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ ë°˜í™˜
        
        Returns:
            List[str]: ì¤‘ë³µ ì œê±°ëœ ì†ŒìŠ¤ ëª©ë¡
        """
        if not self.loaded:
            if not self.load_data():
                return []
                
        sources = set()
        for article in self.articles_data:
            source = article.get('source', '')
            if source:
                sources.add(source)
                
        return sorted(list(sources))
    
    def search_articles(self, query: str, limit: int = 100) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.loaded:
            if not self.load_data():
                return []
                
        query_lower = query.lower()
        results = []
        
        for article in self.articles_data:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            keywords = article.get('keywords', '')
            
            # keywordsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ì†Œë¬¸ìë¡œ ë³€í™˜
            if isinstance(keywords, str):
                keywords = keywords.lower()
            elif isinstance(keywords, list):
                keywords = ' '.join(keywords).lower()
            else:
                keywords = ''
            
            if (query_lower in title or 
                query_lower in summary or 
                query_lower in keywords):
                results.append(article)
                
                if len(results) >= limit:
                    break
                    
        logger.info(f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """
        ë°ì´í„° í†µê³„ ë°˜í™˜
        
        Returns:
            Dict: í†µê³„ ì •ë³´
        """
        if not self.loaded:
            if not self.load_data():
                return {}
                
        sources = self.get_sources()
        
        # ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚° (ìµœê·¼ 7ì¼)
        daily_counts = []
        for i in range(7):
            target_date = datetime.now() - timedelta(days=i)
            count = 0
            
            for article in self.articles_data:
                try:
                    published = article.get('published', '')
                    if published:
                        article_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                        if article_date.date() == target_date.date():
                            count += 1
                except:
                    continue
                    
            daily_counts.append({
                'date': target_date.strftime('%Y-%m-%d'),
                'count': count
            })
        
        daily_counts.reverse()  # ì˜¤ë˜ëœ ë‚ ì§œë¶€í„° ì •ë ¬
        
        return {
            'total_articles': len(self.articles_data),
            'total_sources': len(sources),
            'total_favorites': 0,  # JSON ë°ì´í„°ì—ëŠ” ì¦ê²¨ì°¾ê¸° ì •ë³´ ì—†ìŒ
            'daily_counts': daily_counts,
            'data_source': 'json_file',
            'file_path': str(self.json_file_path)
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
json_loader = JSONDataLoader()