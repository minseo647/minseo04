"""
Hybrid Data Collector
ê¸°ì¡´ JSON íŒŒì¼ë“¤ì„ í™œìš©í•˜ë©´ì„œ RSSë¡œëŠ” ìµœê·¼ 1ì£¼ì¼ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ëŠ” ìµœì í™”ëœ ìˆ˜ì§‘ê¸°
"""

import asyncio
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HybridDataCollector:
    def __init__(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.project_root = Path(__file__).parent.parent
        self.json_files = self._discover_json_files()
        self.cutoff_date = datetime.now() - timedelta(days=7)  # 1ì£¼ì¼ ì œí•œ
        
    def _discover_json_files(self) -> List[Path]:
        """í”„ë¡œì íŠ¸ ë‚´ JSON íŒŒì¼ë“¤ ìë™ ë°œê²¬"""
        json_files = []
        
        # ì•Œë ¤ì§„ JSON íŒŒì¼ë“¤
        known_files = [
            "news_data.json",
            "hankyung_it_20240829_20250829.json", 
            "kbench_articles.json",
            "news_articles.json"
        ]
        
        for file_name in known_files:
            file_path = self.project_root / file_name
            if file_path.exists():
                json_files.append(file_path)
                logger.info(f"ğŸ“ ë°œê²¬ëœ JSON íŒŒì¼: {file_path}")
        
        # ì¶”ê°€ JSON íŒŒì¼ ê²€ìƒ‰ (íŒ¨í„´ ë§¤ì¹­)
        for json_file in self.project_root.glob("*.json"):
            if json_file not in json_files and json_file.name not in [
                "package.json", "tsconfig.json", "vercel.json"
            ]:
                json_files.append(json_file)
                logger.info(f"ğŸ“ ì¶”ê°€ ë°œê²¬ëœ JSON íŒŒì¼: {json_file}")
        
        logger.info(f"ğŸ—‚ï¸ ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
        return json_files

    async def collect_all_data(self) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (JSON íŒŒì¼ + ìµœê·¼ RSS)"""
        start_time = datetime.now()
        logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        try:
            # 1. ê¸°ì¡´ JSON íŒŒì¼ë“¤ ë¡œë“œ
            json_stats = await self._load_json_files()
            
            # 2. ìµœê·¼ 1ì£¼ì¼ RSS ë°ì´í„° ìˆ˜ì§‘
            rss_stats = await self._collect_recent_rss()
            
            # ê²°ê³¼ í†µê³„
            total_stats = {
                'start_time': start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'duration': (datetime.now() - start_time).total_seconds(),
                'json_files': {
                    'loaded_files': json_stats['loaded_files'],
                    'total_articles': json_stats['total_articles'],
                    'inserted': json_stats['inserted'],
                    'skipped': json_stats['skipped'],
                    'failed': json_stats['failed']
                },
                'rss_collection': {
                    'feeds_processed': rss_stats['feeds_processed'],
                    'recent_articles': rss_stats['recent_articles'],
                    'inserted': rss_stats['inserted'],
                    'skipped': rss_stats['skipped']
                },
                'total_inserted': json_stats['inserted'] + rss_stats['inserted'],
                'total_processed': json_stats['total_articles'] + rss_stats['recent_articles']
            }
            
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì™„ë£Œ: JSON {json_stats['inserted']}ê°œ + RSS {rss_stats['inserted']}ê°œ = ì´ {total_stats['total_inserted']}ê°œ ì‹ ê·œ ì¶”ê°€")
            return total_stats
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise

    async def _load_json_files(self) -> Dict[str, Any]:
        """JSON íŒŒì¼ë“¤ì„ DBì— ë¡œë“œ"""
        logger.info("ğŸ“‚ JSON íŒŒì¼ ë¡œë”© ì‹œì‘")
        
        try:
            from json_data_loader import JSONDataLoader
            
            total_stats = {
                'loaded_files': 0,
                'total_articles': 0,
                'inserted': 0,
                'skipped': 0,
                'failed': 0
            }
            
            for json_file in self.json_files:
                logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {json_file.name}")
                
                try:
                    loader = JSONDataLoader(json_file_path=str(json_file))
                    
                    # ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
                    if loader.load_data():
                        articles_count = len(loader.articles_data)
                        logger.info(f"  â””â”€ ë¡œë“œëœ ê¸°ì‚¬ ìˆ˜: {articles_count}")
                        
                        # DBì— ì €ì¥
                        stats = loader.save_articles_to_db()
                        
                        total_stats['loaded_files'] += 1
                        total_stats['total_articles'] += articles_count
                        total_stats['inserted'] += stats.get('inserted', 0)
                        total_stats['skipped'] += stats.get('skipped', 0)
                        total_stats['failed'] += stats.get('failed', 0)
                        
                        logger.info(f"  â””â”€ DB ì €ì¥: {stats['inserted']}ê°œ ì‹ ê·œ, {stats['skipped']}ê°œ ì¤‘ë³µ, {stats['failed']}ê°œ ì‹¤íŒ¨")
                    else:
                        logger.warning(f"  â””â”€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {json_file.name}")
                        
                except Exception as e:
                    logger.error(f"  â””â”€ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {json_file.name}: {e}")
                    continue
            
            logger.info(f"ğŸ“‚ JSON ë¡œë”© ì™„ë£Œ: {total_stats['loaded_files']}ê°œ íŒŒì¼, {total_stats['inserted']}ê°œ ì‹ ê·œ ê¸°ì‚¬")
            return total_stats
            
        except ImportError:
            logger.error("âŒ JSONDataLoader ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'loaded_files': 0, 'total_articles': 0, 'inserted': 0, 'skipped': 0, 'failed': 0}

    async def _collect_recent_rss(self) -> Dict[str, Any]:
        """ìµœê·¼ 1ì£¼ì¼ RSS ë°ì´í„°ë§Œ ìˆ˜ì§‘"""
        logger.info("ğŸ“¡ ìµœê·¼ 1ì£¼ì¼ RSS ìˆ˜ì§‘ ì‹œì‘")
        
        try:
            from weekly_news_collector import collect_weekly_news_async
            
            # ê¸°ì¡´ weekly collectorë¥¼ ì‚¬ìš©í•˜ë˜, ë‚ ì§œ í•„í„°ë§ ê°•í™”
            logger.info(f"ğŸ—“ï¸ ìˆ˜ì§‘ ê¸°ì¤€: {self.cutoff_date.strftime('%Y-%m-%d')} ì´í›„ ê¸°ì‚¬")
            
            # RSS ìˆ˜ì§‘ ì‹¤í–‰
            result = await collect_weekly_news_async()
            
            if result and 'stats' in result:
                stats = result['stats']
                rss_stats = {
                    'feeds_processed': result.get('successful_feeds', 0),
                    'recent_articles': stats.get('total_processed', 0),
                    'inserted': stats.get('total_inserted', 0),
                    'skipped': stats.get('total_skipped', 0)
                }
                
                logger.info(f"ğŸ“¡ RSS ìˆ˜ì§‘ ì™„ë£Œ: {rss_stats['feeds_processed']}ê°œ í”¼ë“œ, {rss_stats['inserted']}ê°œ ì‹ ê·œ")
                return rss_stats
            else:
                logger.warning("RSS ìˆ˜ì§‘ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return {'feeds_processed': 0, 'recent_articles': 0, 'inserted': 0, 'skipped': 0}
                
        except ImportError:
            logger.error("âŒ weekly_news_collector ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'feeds_processed': 0, 'recent_articles': 0, 'inserted': 0, 'skipped': 0}
        except Exception as e:
            logger.error(f"âŒ RSS ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {'feeds_processed': 0, 'recent_articles': 0, 'inserted': 0, 'skipped': 0}

    def get_data_sources_info(self) -> Dict[str, Any]:
        """ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            'json_files': [
                {
                    'name': f.name,
                    'path': str(f),
                    'size_mb': round(f.stat().st_size / 1024 / 1024, 2),
                    'modified': datetime.fromtimestamp(f.stat().st_mtime).isoformat()
                }
                for f in self.json_files
            ],
            'rss_collection_period': f"ìµœê·¼ {7}ì¼",
            'cutoff_date': self.cutoff_date.isoformat()
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
hybrid_collector = HybridDataCollector()

async def collect_hybrid_data_async():
    """ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
    return await hybrid_collector.collect_all_data()

def get_hybrid_collector_info():
    """ìˆ˜ì§‘ê¸° ì •ë³´ ë°˜í™˜"""
    return hybrid_collector.get_data_sources_info()

# CLI ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    try:
        from database import init_db
        logger.info("ğŸ—ƒï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...")
        init_db()
    except Exception as e:
        logger.error(f"DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    try:
        result = await hybrid_collector.collect_all_data()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼")
        print("="*60)
        print(f"â±ï¸  ì†Œìš”ì‹œê°„: {result['duration']:.1f}ì´ˆ")
        print(f"ğŸ“ JSON íŒŒì¼: {result['json_files']['loaded_files']}ê°œ íŒŒì¼ì—ì„œ {result['json_files']['inserted']}ê°œ ì‹ ê·œ ì¶”ê°€")
        print(f"ğŸ“¡ RSS ìˆ˜ì§‘: {result['rss_collection']['feeds_processed']}ê°œ í”¼ë“œì—ì„œ {result['rss_collection']['inserted']}ê°œ ì‹ ê·œ ì¶”ê°€")
        print(f"ğŸ“Š ì´ ê²°ê³¼: {result['total_inserted']}ê°œ ì‹ ê·œ ê¸°ì‚¬ ì¶”ê°€")
        print("="*60)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(main())