"""
Auto Summarizer
ë‰´ìŠ¤ ê¸°ì‚¬ ìë™ ìš”ì•½ ìƒì„± ì‹œìŠ¤í…œ
"""

import requests
from bs4 import BeautifulSoup
import logging
from typing import Optional, Dict, Any
import re
from datetime import datetime
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoSummarizer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def generate_summary_from_title(self, title: str, source: str = "") -> str:
        """
        ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
        """
        try:
            # HTML íƒœê·¸ ì œê±°
            clean_title = re.sub(r'<[^>]+>', '', title)
            # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
            clean_title = re.sub(r'[^\w\sê°€-í£]', ' ', clean_title)
            
            # í•œêµ­ì–´/ì˜ì–´ë³„ ì²˜ë¦¬
            if self._is_korean(clean_title):
                # í•œêµ­ì–´ ìš”ì•½ ìƒì„±
                summary = self._generate_korean_summary(clean_title, source)
            else:
                # ì˜ì–´ ìš”ì•½ ìƒì„±
                summary = self._generate_english_summary(clean_title, source)
            
            return summary
            
        except Exception as e:
            logger.error(f"ì œëª© ê¸°ë°˜ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"{title[:100]}..." if len(title) > 100 else title
    
    def _is_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì—¬ë¶€ íŒë‹¨"""
        korean_count = len(re.findall(r'[ê°€-í£]', text))
        return korean_count > len(text) * 0.1
    
    def _generate_korean_summary(self, title: str, source: str = "") -> str:
        """í•œêµ­ì–´ ì œëª©ì—ì„œ ìš”ì•½ ìƒì„±"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_korean_keywords(title)
        
        # ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜
        news_type = self._classify_korean_news(title)
        
        # ìš”ì•½ í…œí”Œë¦¿
        if "ê¸°ì—…" in news_type or "íˆ¬ì" in news_type:
            summary = f"{', '.join(keywords[:3])} ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ì†Œì‹ì…ë‹ˆë‹¤. "
        elif "ê¸°ìˆ " in news_type or "AI" in keywords:
            summary = f"{', '.join(keywords[:3])} ê¸°ìˆ  ë°œì „ ë° ë™í–¥ì— ê´€í•œ ë¶„ì„ì…ë‹ˆë‹¤. "
        elif "ì •ì±…" in news_type or "ê·œì œ" in news_type:
            summary = f"{', '.join(keywords[:2])} ì •ì±… ë° ê·œì œ ë³€í™”ì— ëŒ€í•œ ë³´ë„ì…ë‹ˆë‹¤. "
        else:
            summary = f"{', '.join(keywords[:2])} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤. "
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        if source:
            summary += f"({source} ë³´ë„)"
        
        return summary
    
    def _generate_english_summary(self, title: str, source: str = "") -> str:
        """ì˜ì–´ ì œëª©ì—ì„œ ìš”ì•½ ìƒì„±"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_english_keywords(title)
        
        # ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜
        if any(word in title.lower() for word in ['launch', 'release', 'announce', 'unveil']):
            summary = f"New {', '.join(keywords[:2])} announcement and launch details. "
        elif any(word in title.lower() for word in ['invest', 'funding', 'raise', 'acquire']):
            summary = f"Investment and business developments involving {', '.join(keywords[:2])}. "
        elif any(word in title.lower() for word in ['ai', 'machine learning', 'tech', 'innovation']):
            summary = f"Latest technology trends and innovations in {', '.join(keywords[:2])}. "
        else:
            summary = f"Latest news and updates on {', '.join(keywords[:2])}. "
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        if source:
            summary += f"(Reported by {source})"
        
        return summary
    
    def _extract_korean_keywords(self, title: str) -> list:
        """í•œêµ­ì–´ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = ['ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'í•˜ê³ ', 'í•˜ëŠ”', 'í•œ', 'í• ', 'í•¨', 'ë°', 'ë“±']
        
        # ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜)
        words = title.split()
        keywords = []
        
        for word in words:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            clean_word = re.sub(r'[^\wê°€-í£]', '', word)
            # ê¸¸ì´ ì²´í¬ ë° ë¶ˆìš©ì–´ ì œê±°
            if len(clean_word) > 1 and clean_word not in stop_words:
                keywords.append(clean_word)
        
        return keywords[:5]
    
    def _extract_english_keywords(self, title: str) -> list:
        """ì˜ì–´ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´
        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may', 'might', 'can'}
        
        # ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'\b[a-zA-Z]+\b', title.lower())
        keywords = []
        
        for word in words:
            if len(word) > 2 and word not in stop_words:
                keywords.append(word.capitalize())
        
        return keywords[:5]
    
    def _classify_korean_news(self, title: str) -> str:
        """í•œêµ­ì–´ ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜"""
        if any(keyword in title for keyword in ['ê¸°ì—…', 'íšŒì‚¬', 'ìƒì¥', 'íˆ¬ì', 'ì¸ìˆ˜', 'í•©ë³‘']):
            return "ê¸°ì—…"
        elif any(keyword in title for keyword in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ê¸°ìˆ ', 'í˜ì‹ ', 'ê°œë°œ']):
            return "ê¸°ìˆ "
        elif any(keyword in title for keyword in ['ì •ë¶€', 'ì •ì±…', 'ê·œì œ', 'ë²•ì•ˆ', 'ì œë„']):
            return "ì •ì±…"
        elif any(keyword in title for keyword in ['ì‹œì¥', 'ì£¼ê°€', 'ê²½ì œ', 'ì‚°ì—…']):
            return "ê²½ì œ"
        else:
            return "ì¼ë°˜"
    
    def scrape_and_summarize(self, url: str, max_length: int = 200) -> Optional[str]:
        """
        ì›¹í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ìš”ì•½ ìƒì„±
        """
        try:
            logger.info(f"ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # ìš”ì²­ ì œí•œ (ì¼ë¶€ ì‚¬ì´íŠ¸ ì°¨ë‹¨ ë°©ì§€)
            time.sleep(1)
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ ì„ íƒì)
            content_selectors = [
                'article',
                '.article-content', 
                '.content', 
                '.post-content',
                '.entry-content',
                '.article-body',
                '.news-content',
                'main p',
                '.container p'
            ]
            
            content_text = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_text = ' '.join([elem.get_text().strip() for elem in elements])
                    break
            
            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ëª¨ë“  p íƒœê·¸ì—ì„œ ì¶”ì¶œ
            if not content_text:
                paragraphs = soup.find_all('p')
                content_text = ' '.join([p.get_text().strip() for p in paragraphs])
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content_text = re.sub(r'\s+', ' ', content_text).strip()
            
            if len(content_text) > 50:
                # ê°„ë‹¨í•œ ë¬¸ì¥ ê¸°ë°˜ ìš”ì•½
                summary = self._extract_key_sentences(content_text, max_length)
                logger.info(f"ìŠ¤í¬ë˜í•‘ ìš”ì•½ ìƒì„± ì„±ê³µ: {len(summary)} ë¬¸ì")
                return summary
            else:
                logger.warning(f"ìŠ¤í¬ë˜í•‘í•œ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(content_text)} ë¬¸ì")
                return None
                
        except requests.RequestException as e:
            logger.warning(f"ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_key_sentences(self, text: str, max_length: int) -> str:
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„±"""
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s+', text)
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ ì œì™¸
        valid_sentences = [s.strip() for s in sentences if 20 <= len(s.strip()) <= 200]
        
        if not valid_sentences:
            return text[:max_length] + "..." if len(text) > max_length else text
        
        # ì²« ë²ˆì§¸ ìœ íš¨í•œ ë¬¸ì¥ë“¤ë¡œ ìš”ì•½ êµ¬ì„±
        summary = ""
        for sentence in valid_sentences[:3]:  # ìµœëŒ€ 3ë¬¸ì¥
            if len(summary + sentence) <= max_length:
                summary += sentence + ". "
            else:
                break
        
        return summary.strip() if summary else text[:max_length] + "..."

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
auto_summarizer = AutoSummarizer()

def generate_auto_summary(title: str, url: str = "", source: str = "") -> str:
    """
    ìë™ ìš”ì•½ ìƒì„± ë©”ì¸ í•¨ìˆ˜
    """
    try:
        # 1ì°¨: ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œë„
        if url and url.startswith('http'):
            scraped_summary = auto_summarizer.scrape_and_summarize(url)
            if scraped_summary and len(scraped_summary) > 30:
                return scraped_summary
        
        # 2ì°¨: ì œëª© ê¸°ë°˜ ìš”ì•½ ìƒì„±
        title_summary = auto_summarizer.generate_summary_from_title(title, source)
        return title_summary
        
    except Exception as e:
        logger.error(f"ìë™ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"{title[:100]}..." if len(title) > 100 else title

# CLI í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    test_cases = [
        {
            "title": "ì‚¼ì„±ì „ì, AI ë°˜ë„ì²´ ì‹ ê¸°ìˆ  ë°œí‘œ",
            "source": "ì „ìì‹ ë¬¸",
            "url": ""
        },
        {
            "title": "OpenAI launches new GPT-5 model with enhanced capabilities",
            "source": "TechCrunch", 
            "url": ""
        }
    ]
    
    print("ğŸ§ª ìë™ ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{i}. ì œëª©: {case['title']}")
        print(f"   ì†ŒìŠ¤: {case['source']}")
        
        summary = generate_auto_summary(case['title'], case['url'], case['source'])
        print(f"   ìš”ì•½: {summary}")