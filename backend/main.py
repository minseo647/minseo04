from fastapi import FastAPI, HTTPException, Query, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Optional, Set, Any
import json
import os
import sys
import logging
import re

# ëŒ€ë¶„ë¥˜/ì†Œë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES: Dict[str, Dict[str, List[str]]] = {
    "ì²¨ë‹¨ ì œì¡°Â·ê¸°ìˆ  ì‚°ì—…": {
        "ë°˜ë„ì²´ ë¶„ì•¼": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "ì‹œìŠ¤í…œ ë°˜ë„ì²´", "íŒŒìš´ë“œë¦¬", "ì†Œì", "ì›¨ì´í¼", "ë…¸ê´‘", "EUV", "ì¥ë¹„", "ì†Œì¬"],
        "ìë™ì°¨ ë¶„ì•¼": ["ìë™ì°¨", "ë‚´ì—°ê¸°ê´€", "ì „ê¸°ì°¨", "ììœ¨ì£¼í–‰", "ëª¨ë¹Œë¦¬í‹°", "í˜„ëŒ€ì°¨", "í…ŒìŠ¬ë¼", "ë°°í„°ë¦¬ì¹´"],
        "ì´ì°¨ì „ì§€ ë¶„ì•¼": ["ì´ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ESS", "ì–‘ê·¹ì¬", "ìŒê·¹ì¬", "ì „í•´ì§ˆ", "ë¶„ë¦¬ë§‰"],
        "ë””ìŠ¤í”Œë ˆì´ ë¶„ì•¼": ["ë””ìŠ¤í”Œë ˆì´", "OLED", "QD", "ë§ˆì´í¬ë¡œ LED", "LCD"],
        "ë¡œë´‡Â·ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ ë¶„ì•¼": ["ë¡œë´‡", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ì‚°ì—…ìë™í™”", "í˜‘ë™ë¡œë´‡"]
    },
    "ì—ë„ˆì§€Â·í™˜ê²½ ì‚°ì—…": {
        "ì—ë„ˆì§€ ë¶„ì•¼": ["ì„ìœ ", "ê°€ìŠ¤", "ì›ìë ¥", "íƒœì–‘ê´‘", "í’ë ¥", "ìˆ˜ì†Œ", "ì‹ ì¬ìƒì—ë„ˆì§€"],
        "í™˜ê²½Â·íƒ„ì†Œì¤‘ë¦½ ë¶„ì•¼": ["íƒ„ì†Œì¤‘ë¦½", "íê¸°ë¬¼", "ì¹œí™˜ê²½", "ìˆ˜ì²˜ë¦¬", "CCUS", "ì¬í™œìš©"]
    },
    "ë””ì§€í„¸Â·ICT ì‚°ì—…": {
        "AI ë¶„ì•¼": ["AI", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìƒì„±í˜•", "ì±—GPT", "ë¡œë³´í‹±ìŠ¤"],
        "ICTÂ·í†µì‹  ë¶„ì•¼": ["5G", "6G", "í†µì‹ ", "ë„¤íŠ¸ì›Œí¬", "ì¸í”„ë¼", "í´ë¼ìš°ë“œ"],
        "ì†Œí”„íŠ¸ì›¨ì–´Â·í”Œë«í¼": ["ì†Œí”„íŠ¸ì›¨ì–´", "ë©”íƒ€ë²„ìŠ¤", "SaaS", "ë³´ì•ˆ", "í•€í…Œí¬", "í”Œë«í¼"]
    },
    "ë°”ì´ì˜¤Â·í—¬ìŠ¤ì¼€ì–´ ì‚°ì—…": {
        "ë°”ì´ì˜¤Â·ì œì•½ ë¶„ì•¼": ["ë°”ì´ì˜¤", "ì œì•½", "ì‹ ì•½", "ë°”ì´ì˜¤ì‹œë°€ëŸ¬", "ì„¸í¬ì¹˜ë£Œì œ", "ìœ ì „ìì¹˜ë£Œì œ"],
        "ì˜ë£Œê¸°ê¸°Â·í—¬ìŠ¤ì¼€ì–´": ["ì˜ë£Œê¸°ê¸°", "í—¬ìŠ¤ì¼€ì–´", "ë””ì§€í„¸ í—¬ìŠ¤", "ì›¨ì–´ëŸ¬ë¸”", "ì›ê²©ì§„ë£Œ"]
    },
    "ì†Œì¬Â·í™”í•™ ì‚°ì—…": {
        "ì²¨ë‹¨ ì†Œì¬": ["íƒ„ì†Œì†Œì¬", "ë‚˜ë…¸ì†Œì¬", "ê³ ë¶„ì", "ë³µí•©ì†Œì¬"],
        "ì •ë°€í™”í•™Â·ì„ìœ í™”í•™": ["ì •ë°€í™”í•™", "ì„ìœ í™”í•™", "ì¼€ë¯¸ì»¬", "íŠ¹ìˆ˜ê°€ìŠ¤", "ë°˜ë„ì²´ìš© ì¼€ë¯¸ì»¬"]
    },
    "ì¸í”„ë¼Â·ê¸°ë°˜ ì‚°ì—…": {
        "ì² ê°•Â·ì¡°ì„ Â·ê±´ì„¤": ["ì² ê°•", "ì¡°ì„ ", "ê±´ì„¤", "ìŠ¤ë§ˆíŠ¸ê±´ì„¤", "ì¹œí™˜ê²½ ì„ ë°•"],
        "ë¬¼ë¥˜Â·ìœ í†µ": ["ë¬¼ë¥˜", "ìœ í†µ", "ì „ììƒê±°ë˜", "ìŠ¤ë§ˆíŠ¸ ë¬¼ë¥˜", "ê³µê¸‰ë§"],
        "ë†ì—…Â·ì‹í’ˆ": ["ë†ì—…", "ìŠ¤ë§ˆíŠ¸íŒœ", "ëŒ€ì²´ì‹í’ˆ", "ì‹í’ˆ"]
    }
}

# Regex for allowed characters in word cloud tokens  
_ALLOWED_TOKEN_RE = re.compile(r"^[\u1100-\u11FF\u3130-\u318F\uAC00-\uD7A3A-Za-z0-9\s\-\+\.\#_/Â·âˆ™:()&%,]+$")

# ë¶ˆìš©ì–´ (ì œê±°í•  ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤)
STOPWORDS = {
    # í•œê¸€ ì¡°ì‚¬, ì–´ë¯¸, ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤
    'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ì€', 'ëŠ”', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ê²Œ', 'í•œí…Œ',
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ì¢‹ì€', 'ë‚˜ìœ', 'í¬ë‹¤', 'ì‘ë‹¤', 'ë§ë‹¤', 'ì ë‹¤',
    'ê·¸', 'ì´', 'ì €', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'ê²ƒ', 'ë“¤', 'ë“±', 'ë°', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ',
    'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•œ', 'ê´€ë ¨', 'ê²½ìš°', 'ë•Œ', 'ì¤‘', 'í›„', 'ì „', 'ë™ì•ˆ', 'ì‚¬ì´',
    'ë…„', 'ì›”', 'ì¼', 'ì‹œê°„', 'ë¶„', 'ì´ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ê³¼ê±°', 'ë¯¸ë˜',
    'í•œêµ­', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ìœ ëŸ½', 'ì•„ì‹œì•„', 'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°',
    'íšŒì‚¬', 'ê¸°ì—…', 'ì—…ì²´', 'ì—…ê³„', 'ì‚°ì—…', 'ë¶„ì•¼', 'ì‹œì¥', 'ê³ ê°', 'ì‚¬ìš©ì', 'ì´ìš©ì', 'ì†Œë¹„ì',
    'ë°œí‘œ', 'ê³µê°œ', 'ì¶œì‹œ', 'ëŸ°ì¹­', 'ì‹œì‘', 'ì¢…ë£Œ', 'ì™„ë£Œ', 'ì§„í–‰', 'ê³„íš', 'ì˜ˆì •', 'ëª©í‘œ', 'ì„±ê³¼', 'ê²°ê³¼',
    
    # ì˜ì–´ ë¶ˆìš©ì–´ (ëŒ€ì†Œë¬¸ì ëª¨ë‘ í¬í•¨)
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
    'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among',
    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
    'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'they',
    'them', 'their', 'we', 'us', 'our', 'you', 'your', 'he', 'him', 'his', 'she', 'her', 'it', 'its',
    'as', 'so', 'if', 'when', 'where', 'who', 'what', 'why', 'how', 'which', 'than', 'then', 'now',
    'here', 'there', 'more', 'most', 'some', 'any', 'all', 'each', 'every', 'other', 'another', 'such',
    'very', 'much', 'many', 'few', 'little', 'big', 'small', 'large', 'long', 'short', 'high', 'low',
    'new', 'old', 'first', 'last', 'next', 'previous', 'same', 'different', 'good', 'bad', 'best', 'better',
    'get', 'got', 'getting', 'make', 'made', 'making', 'take', 'took', 'taken', 'give', 'gave', 'given',
    'come', 'came', 'coming', 'go', 'went', 'going', 'see', 'saw', 'seen', 'know', 'knew', 'known',
    'think', 'thought', 'say', 'said', 'tell', 'told', 'ask', 'asked', 'work', 'worked', 'working',
    'use', 'used', 'using', 'find', 'found', 'look', 'looked', 'looking', 'seem', 'seemed', 'become',
    'became', 'part', 'over', 'back', 'after', 'use', 'her', 'man', 'day', 'get', 'use', 'man', 'new',
    'now', 'way', 'may', 'say', 'each', 'which', 'she', 'two', 'how', 'its', 'who', 'did', 'yes', 'his',
    'been', 'her', 'my', 'more', 'if', 'no', 'do', 'would', 'my', 'so', 'about', 'out', 'many', 'then',
    
    # URL/ì›¹ ê´€ë ¨
    'http', 'https', 'www', 'com', 'org', 'net', 'co', 'kr', 'html', 'url', 'link', 'site', 'web', 'page',
    
    # ê¸°íƒ€ ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤
    'google', 'microsoft', 'apple', 'facebook', 'twitter', 'instagram', 'youtube', 'amazon', 'netflix',
    'like', 'just', 'also', 'only', 'even', 'still', 'well', 'too', 'really', 'actually', 'probably',
    'maybe', 'perhaps', 'quite', 'rather', 'pretty', 'enough', 'almost', 'nearly', 'around', 'about',
    
    # í•œ ê¸€ì ë‹¨ì–´ë“¤ê³¼ ìˆ«ì
    '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'G', 'K', 'M', 'B', 'T', 'P', 'i', 'x', 'v', 'c'
}

# ê³µí•™/ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ í¬í•¨)
TECH_KEYWORDS = set()

# ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
for major_category, subcategories in CATEGORIES.items():
    for subcategory, keywords in subcategories.items():
        TECH_KEYWORDS.update(keywords)

# ì¶”ê°€ ê¸°ìˆ  í‚¤ì›Œë“œ
ADDITIONAL_TECH_KEYWORDS = {
    # AI/ë¨¸ì‹ ëŸ¬ë‹
    'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§', 'CNN', 'RNN', 'LSTM', 'GAN', 'GPT', 'ChatGPT', 'OpenAI',
    'ìì—°ì–´ì²˜ë¦¬', 'NLP', 'ì»´í“¨í„°ë¹„ì „', 'íŒ¨í„´ì¸ì‹', 'ê°•í™”í•™ìŠµ', 'ì§€ë„í•™ìŠµ', 'ë¹„ì§€ë„í•™ìŠµ', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'BERT',
    'LLM', 'ëŒ€í™”í˜•AI', 'ìƒì„±AI', 'ë©€í‹°ëª¨ë‹¬', 'íŒŒì¸íŠœë‹', 'RAG', 'í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§',
    
    # ì†Œí”„íŠ¸ì›¨ì–´/í”„ë¡œê·¸ë˜ë°
    'Python', 'Java', 'JavaScript', 'TypeScript', 'C++', 'C#', 'Go', 'Rust', 'Swift', 'Kotlin', 'React', 'Vue', 'Angular',
    'Node.js', 'Django', 'Flask', 'Spring', 'í”„ë ˆì„ì›Œí¬', 'ë¼ì´ë¸ŒëŸ¬ë¦¬', 'API', 'REST', 'GraphQL', 'SDK',
    'ì˜¤í”ˆì†ŒìŠ¤', 'ê¹ƒí—ˆë¸Œ', 'GitHub', 'ë²„ì „ê´€ë¦¬', 'Git', 'ì½”ë”©', 'í”„ë¡œê·¸ë˜ë°', 'ê°œë°œì', 'ì†Œí”„íŠ¸ì›¨ì–´',
    'DevOps', 'CI/CD', 'ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤', 'ì»¨í…Œì´ë„ˆ', 'Docker', 'Kubernetes',
    
    # í´ë¼ìš°ë“œ/ì¸í”„ë¼  
    'AWS', 'Azure', 'GCP', 'êµ¬ê¸€í´ë¼ìš°ë“œ', 'í´ë¼ìš°ë“œ', 'ì„œë²„ë¦¬ìŠ¤', 'ê°€ìƒí™”', 'VM', 'í•˜ì´ë¸Œë¦¬ë“œí´ë¼ìš°ë“œ', 'ë©€í‹°í´ë¼ìš°ë“œ',
    
    # ë°ì´í„°/ë¹…ë°ì´í„°
    'ë¹…ë°ì´í„°', 'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤', 'ë°ì´í„°ë¶„ì„', 'ë°ì´í„°ë§ˆì´ë‹', 'ë°ì´í„°ë² ì´ìŠ¤', 'SQL', 'NoSQL', 'MongoDB',
    'PostgreSQL', 'MySQL', 'ë°ì´í„°ì›¨ì–´í•˜ìš°ìŠ¤', 'ë°ì´í„°ë ˆì´í¬', 'ETL', 'ELT', 'Apache', 'Hadoop', 'Spark',
    'ë¹„ì¦ˆë‹ˆìŠ¤ì¸í…”ë¦¬ì „ìŠ¤', 'BI', 'ì‹œê°í™”', 'Tableau', 'íŒŒì›ŒBI',
    
    # ë„¤íŠ¸ì›Œí¬/í†µì‹ 
    '5G', '6G', 'LTE', 'WiFi', 'ë¸”ë£¨íˆ¬ìŠ¤', 'IoT', 'ì‚¬ë¬¼ì¸í„°ë„·', 'ë¬´ì„ í†µì‹ ', 'ë„¤íŠ¸ì›Œí¬', 'ë¼ìš°í„°', 'ìŠ¤ìœ„ì¹˜',
    'VPN', 'ë°©í™”ë²½', 'CDN', 'ì—£ì§€ì»´í“¨íŒ…', 'ì—£ì§€', 'ë„¤íŠ¸ì›Œí‚¹', 'í”„ë¡œí† ì½œ', 'TCP', 'UDP', 'HTTP', 'HTTPS',
    
    # ë³´ì•ˆ/ì‚¬ì´ë²„ë³´ì•ˆ
    'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë³´ì•ˆ', 'ì•”í˜¸í™”', 'í•´í‚¹', 'í”¼ì‹±', 'ëœì„¬ì›¨ì–´', 'ë©€ì›¨ì–´', 'ë°”ì´ëŸ¬ìŠ¤', 'ì·¨ì•½ì ', 'CISO',
    'ì¸ì¦', 'ê¶Œí•œ', 'SSO', 'ë‹¤ì¤‘ì¸ì¦', 'MFA', 'ë¸”ë¡ì²´ì¸', 'ìŠ¤ë§ˆíŠ¸ì»¨íŠ¸ë™íŠ¸', 'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€',
    'ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸', 'ì¹¨ì…íƒì§€', 'ë³´ì•ˆìš´ì˜ì„¼í„°', 'SOC',
    
    # í•˜ë“œì›¨ì–´/ë°˜ë„ì²´
    'ë°˜ë„ì²´', 'ì¹©', 'í”„ë¡œì„¸ì„œ', 'CPU', 'GPU', 'NPU', 'TPU', 'ë©”ëª¨ë¦¬', 'RAM', 'SSD', 'HDD',
    'ì›¨ì´í¼', 'íŒŒìš´ë“œë¦¬', 'TSMC', 'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì¸í…”', 'AMD', 'NVIDIA', 'í€„ì»´',
    'OLED', 'ë””ìŠ¤í”Œë ˆì´', 'LCD', 'ë§ˆì´í¬ë¡œLED', 'ì„¼ì„œ', 'ì¹´ë©”ë¼ëª¨ë“ˆ',
    
    # ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°
    'ì „ê¸°ì°¨', 'EV', 'ììœ¨ì£¼í–‰', 'í…ŒìŠ¬ë¼', 'í˜„ëŒ€ì°¨', 'ê¸°ì•„', 'ëª¨ë¹Œë¦¬í‹°', 'ë°°í„°ë¦¬', 'ë¦¬íŠ¬ë°°í„°ë¦¬',
    'LiDAR', 'ë ˆì´ë”', 'ì»¤ë„¥í‹°ë“œì¹´', 'V2X', 'ì¶©ì „ì¸í”„ë¼', 'ê¸‰ì†ì¶©ì „',
    
    # ë¡œë´‡/ìë™í™”
    'ë¡œë´‡', 'ë¡œë³´í‹±ìŠ¤', 'ìë™í™”', 'RPA', 'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬', 'ì‚°ì—…ìë™í™”', 'í˜‘ë™ë¡œë´‡', 'ë“œë¡ ', 'UAV',
    '3Dí”„ë¦°íŒ…', 'ì ì¸µì œì¡°', 'CNC', 'ìŠ¤ë§ˆíŠ¸ì œì¡°',
    
    # ë°”ì´ì˜¤/í—¬ìŠ¤ì¼€ì–´
    'ë°”ì´ì˜¤', 'ë°”ì´ì˜¤í…Œí¬', 'ì œì•½', 'ì‹ ì•½ê°œë°œ', 'ìœ ì „ì', 'DNA', 'RNA', 'mRNA', 'ë°±ì‹ ', 'í•­ì²´',
    'ì˜ë£Œê¸°ê¸°', 'ë””ì§€í„¸í—¬ìŠ¤', 'í—¬ìŠ¤ì¼€ì–´', 'ì›ê²©ì˜ë£Œ', 'í…”ë ˆí—¬ìŠ¤', 'ì›¨ì–´ëŸ¬ë¸”', 'ë°”ì´ì˜¤ì„¼ì„œ',
    'ì •ë°€ì˜í•™', 'ê°œì¸ë§ì¶¤ì˜ë£Œ', 'AIì§„ë‹¨', 'ì˜ë£ŒAI',
    
    # ì—ë„ˆì§€/í™˜ê²½
    'íƒœì–‘ê´‘', 'í’ë ¥', 'ìˆ˜ì†Œ', 'ì—°ë£Œì „ì§€', 'ì—ë„ˆì§€ì €ì¥ì¥ì¹˜', 'ESS', 'ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ', 'ì‹ ì¬ìƒì—ë„ˆì§€',
    'íƒ„ì†Œì¤‘ë¦½', 'íƒ„ì†Œí¬ì§‘', 'CCUS', 'ì¹œí™˜ê²½', 'ê·¸ë¦°í…Œí¬', 'ì²­ì •ê¸°ìˆ ',
    
    # ê²Œì„/ì—”í„°í…Œì¸ë¨¼íŠ¸
    'ê²Œì„ì—”ì§„', 'Unity', 'Unreal', 'VR', 'ê°€ìƒí˜„ì‹¤', 'AR', 'ì¦ê°•í˜„ì‹¤', 'MR', 'í˜¼í•©í˜„ì‹¤', 'ë©”íƒ€ë²„ìŠ¤',
    'NFT', 'P2E', 'ê²Œì„ê°œë°œ', 'ëª¨ë°”ì¼ê²Œì„', 'PCê²Œì„', 'ì½˜ì†”ê²Œì„',
    
    # í•€í…Œí¬/ê¸ˆìœµê¸°ìˆ 
    'í•€í…Œí¬', 'ë””ì§€í„¸ë±…í‚¹', 'ëª¨ë°”ì¼í˜ì´', 'ê°„í¸ê²°ì œ', 'CBDC', 'ë””ì§€í„¸í™”í', 'í¬ë¼ìš°ë“œí€ë”©',
    'ë¡œë³´ì–´ë“œë°”ì´ì €', 'ì¸ìŠˆì–´í…Œí¬', 'RegTech', 'ë””ì§€í„¸ê¸ˆìœµ',
    
    # ì‹ ê¸°ìˆ 
    'ì–‘ìì»´í“¨íŒ…', 'ì–‘ì', 'ë‚˜ë…¸ê¸°ìˆ ', 'ì‹ ì†Œì¬', 'íƒ„ì†Œë‚˜ë…¸íŠœë¸Œ', 'ê·¸ë˜í•€', 'ìŠˆí¼ì»´í“¨í„°', 'HPC',
    'ì—£ì§€AI', 'ì˜¨ë””ë°”ì´ìŠ¤AI', 'TinyML', 'ë””ì§€í„¸íŠ¸ìœˆ', 'APIê²½ì œ', 'SaaS', 'PaaS', 'IaaS'
}

# ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œ í•©ì¹˜ê¸°
TECH_KEYWORDS.update(ADDITIONAL_TECH_KEYWORDS)

def is_meaningful_token(token: str) -> bool:
    """ì˜ë¯¸ìˆëŠ” í† í°ì¸ì§€ í™•ì¸"""
    token = str(token).strip()
    
    # ê¸¸ì´ ì²´í¬ (1ê¸€ì ì œì™¸, ë‹¨ ì˜ë¬¸ ì•½ì–´ëŠ” í—ˆìš©)
    if len(token) <= 1:
        return False
    
    # í•œê¸€ 1ê¸€ìëŠ” ì œì™¸
    if len(token) == 1 and '\uAC00' <= token <= '\uD7A3':
        return False
    
    # ë¶ˆìš©ì–´ ì²´í¬
    if token.lower() in STOPWORDS:
        return False
    
    # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸ (ë‹¨, ê¸°ìˆ  ê´€ë ¨ ìˆ«ìëŠ” í—ˆìš©)
    if token.isdigit() and token not in TECH_KEYWORDS:
        return False
    
    return True

def is_tech_term(token: str) -> bool:
    """ê¸°ìˆ  ê´€ë ¨ ìš©ì–´ì¸ì§€ í™•ì¸"""
    token = str(token).strip()
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš°
    if token in TECH_KEYWORDS:
        return True
    
    # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì²´í¬
    if token.lower() in {kw.lower() for kw in TECH_KEYWORDS}:
        return True
    
    # ë¶€ë¶„ ë§¤ì¹­ (ê¸°ìˆ  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°)
    for tech_kw in TECH_KEYWORDS:
        if tech_kw.lower() in token.lower() or token.lower() in tech_kw.lower():
            return True
    
    return False

def _guess_korean_font_path(user_font_path: Optional[str] = None) -> Optional[str]:
    """í•œê¸€ í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ëŠ” í•¨ìˆ˜ - ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›"""
    if user_font_path and os.path.exists(user_font_path): 
        return user_font_path
    
    # í•œê¸€ í°íŠ¸ í›„ë³´ë“¤ (ìš°ì„ ìˆœìœ„ëŒ€ë¡œ)
    candidates = [
        # Windows í°íŠ¸
        r"C:\Windows\Fonts\malgun.ttf",
        r"C:\Windows\Fonts\malgunbd.ttf", 
        r"C:\Windows\Fonts\NanumGothic.ttf",
        r"C:\Windows\Fonts\NanumBarunGothic.ttf",
        r"C:\Windows\Fonts\NotoSansKR-Regular.otf",
        r"C:\Windows\Fonts\NotoSansCJKkr-Regular.otf",
        r"C:\Windows\Fonts\gulim.ttc",
        r"C:\Windows\Fonts\batang.ttc",
        
        # macOS í°íŠ¸
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/System/Library/Fonts/AppleGothic.ttf",
        "/System/Library/Fonts/Supplemental/AppleGothic.ttf",
        "/Library/Fonts/AppleGothic.ttf",
        "/Library/Fonts/NanumGothic.otf",
        "/Library/Fonts/NanumGothic.ttf",
        "/Library/Fonts/NotoSansKR-Regular.otf",
        "/System/Library/Fonts/Helvetica.ttc",
        
        # Linux/Ubuntu í°íŠ¸
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf", 
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansKR-Regular.otf",
        "/usr/share/fonts/truetype/noto/NotoSansKR-Regular.ttf",
        "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        "/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf",
        
        # ì¶”ê°€ Linux ê²½ë¡œë“¤
        "/usr/share/fonts/TTF/NanumGothic.ttf",
        "/usr/share/fonts/OTF/NotoSansKR-Regular.otf",
        "/usr/local/share/fonts/NanumGothic.ttf", 
        "/home/fonts/NanumGothic.ttf",
        
        # Docker/ì»¨í…Œì´ë„ˆ í™˜ê²½
        "/fonts/NanumGothic.ttf",
        "/app/fonts/NanumGothic.ttf",
        "./fonts/NanumGothic.ttf",
        
        # Google Fonts (ì›¹ í™˜ê²½)
        "/tmp/NanumGothic.ttf",
        "/var/tmp/NanumGothic.ttf"
    ]
    
    for path in candidates:
        if os.path.exists(path): 
            logger.info(f"âœ… í•œê¸€ í°íŠ¸ ë°œê²¬: {path}")
            return path
    
    # ì‹œìŠ¤í…œì—ì„œ í°íŠ¸ ê²€ìƒ‰ ì‹œë„
    try:
        import subprocess
        
        # Linux/Unix ì‹œìŠ¤í…œì—ì„œ fc-listë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ í°íŠ¸ ì°¾ê¸°
        result = subprocess.run(['fc-list', ':lang=ko'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0 and result.stdout:
            for line in result.stdout.split('\n'):
                if line and '.ttf' in line.lower():
                    font_path = line.split(':')[0].strip()
                    if os.path.exists(font_path):
                        logger.info(f"âœ… fc-listë¡œ í•œê¸€ í°íŠ¸ ë°œê²¬: {font_path}")
                        return font_path
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception) as e:
        logger.debug(f"fc-list ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    # ë§ˆì§€ë§‰ìœ¼ë¡œ matplotlib ê¸°ë³¸ í°íŠ¸ ì‹œë„
    try:
        import matplotlib.font_manager as fm
        font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')
        korean_fonts = []
        
        for font_path in font_list:
            font_name = os.path.basename(font_path).lower()
            if any(keyword in font_name for keyword in ['nanum', 'malgun', 'gothic', 'noto']):
                korean_fonts.append(font_path)
        
        if korean_fonts:
            selected_font = korean_fonts[0]
            logger.info(f"âœ… matplotlibìœ¼ë¡œ í•œê¸€ í°íŠ¸ ë°œê²¬: {selected_font}")
            return selected_font
            
    except Exception as e:
        logger.debug(f"matplotlib í°íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    logger.warning("âŒ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œì—ì„œ í•œê¸€ì´ ê¹¨ì ¸ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return None

def _filter_wc_tokens(keywords_freq: List[tuple], strict_filter: bool = True) -> List[tuple]:
    """Filter wordcloud tokens based on allowed patterns and meaningfulness"""
    if not keywords_freq: return []
    cleaned = []
    for k, v in keywords_freq:
        ks = str(k).strip()
        if not ks: continue
        if _ALLOWED_TOKEN_RE.match(ks) is None: continue
        if not is_meaningful_token(ks): continue
        if strict_filter and not is_tech_term(ks): continue
        cleaned.append((ks, int(v)))
    return cleaned

def render_wordcloud_wc(keywords_freq: List[tuple], font_path: Optional[str] = None, 
                       auto_korean_font: bool = True, filter_unrenderables: bool = True):
    """Render wordcloud with Korean font support"""
    if not keywords_freq:
        return None
        
    filtered = _filter_wc_tokens(keywords_freq, strict_filter=filter_unrenderables)
    if not filtered:
        return None
        
    fp = None
    if auto_korean_font:
        fp = _guess_korean_font_path(font_path)
        
        # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ë‹¤ìš´ë¡œë“œ ì‹œë„
        if fp is None:
            fp = _download_korean_font()
    else:
        fp = font_path
    
    try:
        from wordcloud import WordCloud
        import matplotlib.pyplot as plt
        
        # í•œê¸€ í°íŠ¸ ì§€ì›ì„ ìœ„í•œ matplotlib ì„¤ì •
        if fp:
            try:
                import matplotlib.font_manager as fm
                fm.fontManager.addfont(fp)
                plt.rcParams['font.family'] = [fp]
            except Exception as e:
                logger.debug(f"matplotlib í°íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        wc = WordCloud(
            width=1000, height=500,
            background_color="white",
            collocations=False,
            font_path=fp,
            max_words=200,
            relative_scaling=0.5,
            colormap='viridis'
        )
        wc.generate_from_frequencies({k: int(v) for k, v in filtered})
        
        # Return the wordcloud object for further processing
        return wc, fp
        
    except ImportError:
        logger.error("WordCloud library not available")
        return None

def _download_korean_font() -> Optional[str]:
    """ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì„ì‹œ ë””ë ‰í„°ë¦¬ì— ì €ì¥"""
    try:
        import urllib.request
        
        # í°íŠ¸ ì €ì¥í•  ì„ì‹œ ë””ë ‰í„°ë¦¬ ìƒì„±
        font_dir = "/tmp/fonts"
        os.makedirs(font_dir, exist_ok=True)
        font_path = os.path.join(font_dir, "NanumGothic.ttf")
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš°
        if os.path.exists(font_path):
            logger.info(f"âœ… ê¸°ì¡´ ë‹¤ìš´ë¡œë“œëœ í•œê¸€ í°íŠ¸ ì‚¬ìš©: {font_path}")
            return font_path
        
        # ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ë‹¤ìš´ë¡œë“œ (Google Fonts)
        font_url = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Regular.ttf"
        
        logger.info("ğŸ“¥ í•œê¸€ í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        urllib.request.urlretrieve(font_url, font_path)
        
        if os.path.exists(font_path) and os.path.getsize(font_path) > 1000:  # 1KB ì´ìƒ
            logger.info(f"âœ… í•œê¸€ í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {font_path}")
            return font_path
        else:
            logger.error("âŒ ë‹¤ìš´ë¡œë“œëœ í°íŠ¸ íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            if os.path.exists(font_path):
                os.remove(font_path)
            return None
            
    except Exception as e:
        logger.error(f"âŒ í•œê¸€ í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

from pathlib import Path
from datetime import datetime, timedelta
import asyncio
import base64
import io
from collections import Counter

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Import enhanced modules (updated to include hybrid collector and auto summarizer)
try:
    from database import db, init_db, get_db_connection
    from enhanced_news_collector import collector, collect_news_async
    from weekly_news_collector import collect_weekly_news_async
    from hybrid_data_collector import collect_hybrid_data_async, get_hybrid_collector_info
    from json_data_loader import json_loader
    from auto_summarizer import generate_auto_summary
    ENHANCED_MODULES_AVAILABLE = True
    logger.info("âœ… Enhanced modules loaded successfully (including auto summarizer)")
except ImportError as e:
    logger.error(f"âŒ Failed to load enhanced modules: {e}")
    ENHANCED_MODULES_AVAILABLE = False

# Fallback imports
if not ENHANCED_MODULES_AVAILABLE:
    logger.info("ğŸ”„ Using fallback modules")
    try:
        from simple_news_collector import collect_all_feeds, FEEDS
        SIMPLE_COLLECTOR_AVAILABLE = True
    except ImportError:
        SIMPLE_COLLECTOR_AVAILABLE = False
        logger.error("âŒ No news collector available")

app = FastAPI(
    title="News IT's Issue API",
    description="Enhanced IT/Tech News Collection and Analysis Platform",
    version="2.0.0"
)

# Environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DATABASE_URL = os.getenv("DATABASE_URL", "")
ENABLE_CORS = os.getenv("ENABLE_CORS", "true").lower() == "true"

# CORS configuration
if ENABLE_CORS:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Database initialization
_db_initialized = False

async def ensure_db_initialized():
    """Ensure database is initialized"""
    global _db_initialized
    if not _db_initialized:
        try:
            if ENHANCED_MODULES_AVAILABLE:
                db.init_database()
            else:
                # Fallback initialization
                import sqlite3
                conn = sqlite3.connect("/tmp/news.db")
                cursor = conn.cursor()
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        title TEXT NOT NULL,
                        link TEXT UNIQUE NOT NULL,
                        published TEXT,
                        source TEXT,
                        summary TEXT,
                        keywords TEXT,
                        created_at TEXT DEFAULT (datetime('now'))
                    )
                """)
                conn.commit()
                conn.close()
            _db_initialized = True
            logger.info("âœ… Database initialized successfully")
        except Exception as e:
            logger.error(f"âŒ Database initialization failed: {e}")
            raise HTTPException(status_code=500, detail="Database initialization failed")

@app.on_event("startup")
async def startup_event():
    """Application startup event"""
    logger.info("ğŸš€ Starting News IT's Issue API Server")
    await ensure_db_initialized()
    
    # Log configuration
    logger.info(f"Database type: {db.db_type if ENHANCED_MODULES_AVAILABLE else 'SQLite'}")
    logger.info(f"Enhanced modules: {'Available' if ENHANCED_MODULES_AVAILABLE else 'Not Available'}")
    logger.info(f"OpenAI API: {'Configured' if OPENAI_API_KEY else 'Not Configured'}")
    logger.info(f"PostgreSQL: {'Available' if DATABASE_URL else 'Not Available'}")

class Article(BaseModel):
    id: int
    title: str
    link: str
    published: str
    source: str
    summary: Optional[str]
    keywords: Optional[str]
    created_at: Optional[str]
    is_favorite: bool = False

class FavoriteRequest(BaseModel):
    article_id: int

class KeywordStats(BaseModel):
    keyword: str
    count: int

class NetworkNode(BaseModel):
    id: str
    label: str
    value: int

class NetworkEdge(BaseModel):
    from_node: str = None
    to: str
    value: int

    model_config = {"field_alias_generator": None}
    
    def dict(self, **kwargs):
        data = super().model_dump(**kwargs)
        if 'from_node' in data:
            data['from'] = data.pop('from_node')
        return data

class CollectionRequest(BaseModel):
    name: str
    rules: Optional[Dict] = None

class NewsCollectionRequest(BaseModel):
    days: int = 30
    max_pages: int = 5

# get_db_connection is now imported from database module

@app.get("/api/articles")
async def get_articles(
    limit: int = Query(100, le=2000),
    offset: int = Query(0, ge=0),
    source: Optional[str] = None,
    search: Optional[str] = None,
    favorites_only: bool = False,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    major_category: Optional[str] = None,
    minor_category: Optional[str] = None,
    use_json: bool = Query(True, description="Use JSON data as default")
):
    """Get articles with filtering and pagination (JSON data by default)"""
    
    try:
        # ê¸°ë³¸ì ìœ¼ë¡œ JSON ë°ì´í„° ì‚¬ìš©
        if use_json:
            logger.info("ğŸ“– Using JSON data source")
            
            # ê²€ìƒ‰ì´ ìˆëŠ” ê²½ìš°
            if search:
                articles = json_loader.search_articles(search, limit * 2)  # ì—¬ìœ ë¶„ìœ¼ë¡œ ë” ë§ì´ ê°€ì ¸ì˜´
            else:
                articles = json_loader.get_articles(limit + offset, 0)
            
            # ì†ŒìŠ¤ í•„í„°ë§
            if source:
                articles = [a for a in articles if a.get('source') == source]
            
            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            if major_category or minor_category:
                def article_matches_category(article):
                    article_text = f"{article.get('title', '')} {article.get('summary', '')} {article.get('keywords', '')}".lower()
                    
                    # ì†Œë¶„ë¥˜ê°€ ì„ íƒëœ ê²½ìš°
                    if minor_category and minor_category in CATEGORIES.get(major_category or '', {}):
                        # ëŒ€ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ ëª¨ë‘ ì„ íƒëœ ê²½ìš°
                        keywords = CATEGORIES[major_category][minor_category]
                    elif minor_category:
                        # ì†Œë¶„ë¥˜ë§Œ ì„ íƒëœ ê²½ìš° (ëª¨ë“  ëŒ€ë¶„ë¥˜ì—ì„œ ì°¾ê¸°)
                        keywords = []
                        for major_cat in CATEGORIES.values():
                            if minor_category in major_cat:
                                keywords = major_cat[minor_category]
                                break
                    elif major_category:
                        # ëŒ€ë¶„ë¥˜ë§Œ ì„ íƒëœ ê²½ìš° (í•´ë‹¹ ëŒ€ë¶„ë¥˜ì˜ ëª¨ë“  í‚¤ì›Œë“œ)
                        keywords = []
                        for minor_cat in CATEGORIES[major_category].values():
                            keywords.extend(minor_cat)
                    else:
                        return True
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­
                    return any(keyword.lower() in article_text for keyword in keywords)
                
                articles = [a for a in articles if article_matches_category(a)]
            
            # ì¦ê²¨ì°¾ê¸° í•„í„°ë§ (JSON ë°ì´í„°ì—ì„œëŠ” DBì˜ ì¦ê²¨ì°¾ê¸° ì •ë³´ì™€ ê²°í•©)
            if favorites_only:
                await ensure_db_initialized()
                if ENHANCED_MODULES_AVAILABLE:
                    favorite_articles = db.get_articles_with_filters(
                        limit=limit*10, offset=0, favorites_only=True
                    )
                    favorite_links = {a.get('link') for a in favorite_articles}
                    articles = [a for a in articles if a.get('link') in favorite_links]
                else:
                    articles = []  # JSON ì „ìš© ëª¨ë“œì—ì„œëŠ” ì¦ê²¨ì°¾ê¸° ì§€ì› ì•ˆí•¨
            
            # is_favorite í•„ë“œ ì¶”ê°€ (DBì—ì„œ ì¦ê²¨ì°¾ê¸° ìƒíƒœ í™•ì¸)
            if ENHANCED_MODULES_AVAILABLE:
                try:
                    await ensure_db_initialized()
                    for article in articles:
                        try:
                            favorites = db.execute_query(
                                "SELECT article_id FROM favorites f JOIN articles a ON f.article_id = a.id WHERE a.link = %s"
                                if db.db_type == "postgresql" 
                                else "SELECT article_id FROM favorites f JOIN articles a ON f.article_id = a.id WHERE a.link = ?",
                                [article.get('link', '')]
                            )
                            article['is_favorite'] = len(favorites) > 0
                        except:
                            article['is_favorite'] = False
                except:
                    for article in articles:
                        article['is_favorite'] = False
            else:
                for article in articles:
                    article['is_favorite'] = False
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
            total_before_pagination = len(articles)
            articles = articles[offset:offset + limit]
            
            logger.info(f"ğŸ“– JSON ë°ì´í„°ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°˜í™˜ (ì „ì²´ {total_before_pagination}ê°œ ì¤‘)")
            return articles
        
        # DB ë°ì´í„° ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)
        else:
            await ensure_db_initialized()
            
            if ENHANCED_MODULES_AVAILABLE:
                articles = db.get_articles_with_filters(
                    limit=limit,
                    offset=offset,
                    source=source,
                    search=search,
                    favorites_only=favorites_only,
                    date_from=date_from,
                    date_to=date_to
                )
                return articles
            else:
                # Fallback implementation
                import sqlite3
                conn = sqlite3.connect("/tmp/news.db")
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                
                query = "SELECT *, 0 as is_favorite FROM articles WHERE 1=1"
                params = []
                
                if source:
                    query += " AND source = ?"
                    params.append(source)
                
                if search:
                    query += " AND (title LIKE ? OR summary LIKE ? OR keywords LIKE ?)"
                    search_param = f"%{search}%"
                    params.extend([search_param, search_param, search_param])
                
                query += " ORDER BY published DESC LIMIT ? OFFSET ?"
                params.extend([limit, offset])
                
                cursor.execute(query, params)
                articles = [dict(row) for row in cursor.fetchall()]
                conn.close()
                
                return articles
            
    except Exception as e:
        logger.error(f"Error fetching articles: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/sources")
async def get_sources(use_json: bool = Query(True, description="Use JSON data as default")):
    """Get available news sources"""
    try:
        # ê¸°ë³¸ì ìœ¼ë¡œ JSON ë°ì´í„° ì‚¬ìš©
        if use_json:
            sources = json_loader.get_sources()
            logger.info(f"ğŸ“– JSONì—ì„œ {len(sources)}ê°œ ì†ŒìŠ¤ ë°˜í™˜")
            return sources
        else:
            # DBì—ì„œ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT DISTINCT source FROM articles ORDER BY source")
            sources = [row[0] for row in cursor.fetchall()]
            conn.close()
            return sources
    except Exception as e:
        logger.error(f"Error fetching sources: {e}")
        return []

@app.get("/api/categories")
async def get_categories():
    """Get available categories (major and minor categories)"""
    try:
        return {
            "categories": CATEGORIES,
            "major_categories": list(CATEGORIES.keys()),
            "all_minor_categories": {
                minor_cat: keywords 
                for major_cat in CATEGORIES.values() 
                for minor_cat, keywords in major_cat.items()
            }
        }
    except Exception as e:
        logger.error(f"Error fetching categories: {e}")
        return {"categories": {}, "major_categories": [], "all_minor_categories": {}}

@app.get("/api/insights")
async def get_insights(
    period: str = Query("daily", description="Time period: daily, weekly, monthly"),
    days_back: int = Query(30, description="Number of days to look back"),
    use_json: bool = Query(True, description="Use JSON data as default")
):
    """Get insights data for charts"""
    try:
        from datetime import datetime, timedelta
        from collections import defaultdict
        
        # ê¸°ë³¸ì ìœ¼ë¡œ JSON ë°ì´í„° ì‚¬ìš©
        if use_json:
            articles = json_loader.get_articles(limit=10000)  # ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°
            
            if not articles:
                return {
                    "time_series": [],
                    "category_counts": {},
                    "total_articles": 0,
                    "period": period
                }
            
            # ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
            time_series_data = defaultdict(int)
            category_counts = defaultdict(int)
            
            from datetime import timezone
            end_date = datetime.now(timezone.utc)
            start_date = end_date - timedelta(days=days_back)
            
            for article in articles:
                try:
                    # ê¸°ì‚¬ ë‚ ì§œ íŒŒì‹±
                    article_date = datetime.fromisoformat(article.get('published', '').replace('Z', '+00:00'))
                    
                    # ì§€ì •ëœ ê¸°ê°„ ë‚´ì˜ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
                    if start_date <= article_date <= end_date:
                        # ì‹œê°„ ë‹¨ìœ„ë³„ ê·¸ë£¹í•‘
                        if period == "daily":
                            date_key = article_date.strftime('%Y-%m-%d')
                        elif period == "weekly":
                            # ì£¼ ì‹œì‘ì¼ (ì›”ìš”ì¼)ë¡œ ê·¸ë£¹í•‘
                            week_start = article_date - timedelta(days=article_date.weekday())
                            date_key = week_start.strftime('%Y-%m-%d')
                        elif period == "monthly":
                            date_key = article_date.strftime('%Y-%m')
                        else:
                            date_key = article_date.strftime('%Y-%m-%d')
                        
                        time_series_data[date_key] += 1
                        
                        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                        article_text = f"{article.get('title', '')} {article.get('summary', '')} {article.get('keywords', '')}".lower()
                        
                        # ê° ëŒ€ë¶„ë¥˜ì— ëŒ€í•´ ë§¤ì¹­ í™•ì¸
                        matched_category = None
                        for major_category, subcategories in CATEGORIES.items():
                            for subcategory, keywords in subcategories.items():
                                if any(keyword.lower() in article_text for keyword in keywords):
                                    matched_category = major_category
                                    break
                            if matched_category:
                                break
                        
                        if matched_category:
                            category_counts[matched_category] += 1
                        else:
                            category_counts["ê¸°íƒ€"] += 1
                            
                except Exception as e:
                    logger.debug(f"Error processing article date: {e}")
                    continue
            
            # ì‹œê³„ì—´ ë°ì´í„° ì •ë ¬
            sorted_time_series = []
            if time_series_data:
                sorted_dates = sorted(time_series_data.keys())
                for date_key in sorted_dates:
                    sorted_time_series.append({
                        "date": date_key,
                        "count": time_series_data[date_key]
                    })
            
            return {
                "time_series": sorted_time_series,
                "category_counts": dict(category_counts),
                "total_articles": sum(time_series_data.values()),
                "period": period,
                "date_range": {
                    "start": start_date.strftime('%Y-%m-%d'),
                    "end": end_date.strftime('%Y-%m-%d')
                }
            }
        
        else:
            # DB ë°ì´í„° ì‚¬ìš©
            await ensure_db_initialized()
            
            if not ENHANCED_MODULES_AVAILABLE:
                return {"time_series": [], "category_counts": {}, "total_articles": 0, "period": period}
            
            # DBì—ì„œ ìµœê·¼ ê¸°ì‚¬ë“¤ ê°€ì ¸ì˜¤ê¸°
            if db.db_type == "postgresql":
                query = """
                    SELECT title, summary, keywords, published, created_at 
                    FROM articles 
                    WHERE created_at >= %s 
                    ORDER BY created_at DESC
                """
                params = (datetime.now(timezone.utc) - timedelta(days=days_back),)
            else:
                query = """
                    SELECT title, summary, keywords, published, created_at 
                    FROM articles 
                    WHERE created_at >= datetime('now', '-{} days') 
                    ORDER BY created_at DESC
                """.format(days_back)
                params = ()
            
            articles = db.execute_query(query, params)
            
            time_series_data = defaultdict(int)
            category_counts = defaultdict(int)
            
            for article in articles:
                try:
                    # created_at ë˜ëŠ” published ì‚¬ìš©
                    date_str = article.get('created_at') or article.get('published')
                    if date_str:
                        article_date = datetime.fromisoformat(str(date_str).replace('Z', '+00:00'))
                        
                        # ì‹œê°„ ë‹¨ìœ„ë³„ ê·¸ë£¹í•‘
                        if period == "daily":
                            date_key = article_date.strftime('%Y-%m-%d')
                        elif period == "weekly":
                            week_start = article_date - timedelta(days=article_date.weekday())
                            date_key = week_start.strftime('%Y-%m-%d')
                        elif period == "monthly":
                            date_key = article_date.strftime('%Y-%m')
                        else:
                            date_key = article_date.strftime('%Y-%m-%d')
                        
                        time_series_data[date_key] += 1
                        
                        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                        article_text = f"{article.get('title', '')} {article.get('summary', '')} {article.get('keywords', '')}".lower()
                        
                        matched_category = None
                        for major_category, subcategories in CATEGORIES.items():
                            for subcategory, keywords in subcategories.items():
                                if any(keyword.lower() in article_text for keyword in keywords):
                                    matched_category = major_category
                                    break
                            if matched_category:
                                break
                        
                        if matched_category:
                            category_counts[matched_category] += 1
                        else:
                            category_counts["ê¸°íƒ€"] += 1
                            
                except Exception as e:
                    logger.debug(f"Error processing article: {e}")
                    continue
            
            # ê²°ê³¼ ì •ë ¬
            sorted_time_series = []
            if time_series_data:
                sorted_dates = sorted(time_series_data.keys())
                for date_key in sorted_dates:
                    sorted_time_series.append({
                        "date": date_key,
                        "count": time_series_data[date_key]
                    })
            
            return {
                "time_series": sorted_time_series,
                "category_counts": dict(category_counts),
                "total_articles": sum(time_series_data.values()),
                "period": period,
                "date_range": {
                    "start": (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime('%Y-%m-%d'),
                    "end": datetime.now(timezone.utc).strftime('%Y-%m-%d')
                }
            }
            
    except Exception as e:
        logger.error(f"Error getting insights: {e}")
        return {
            "time_series": [],
            "category_counts": {},
            "total_articles": 0,
            "period": period,
            "error": str(e)
        }

@app.get("/api/keywords/stats")
async def get_keyword_stats(limit: int = Query(50, le=200), use_json: bool = Query(True, description="Use JSON data as default")):
    """Get keyword statistics"""
    
    try:
        # ê¸°ë³¸ì ìœ¼ë¡œ JSON ë°ì´í„° ì‚¬ìš©
        if use_json:
            logger.info("ğŸ“– JSON ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ í†µê³„ ìƒì„±")
            articles = json_loader.get_articles(limit * 10)  # ì¶©ë¶„í•œ ì–‘ì˜ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
            
            keyword_counter = {}
            for article in articles:
                keywords_str = article.get('keywords', '')
                if keywords_str:
                    try:
                        # JSON í˜•íƒœì˜ í‚¤ì›Œë“œ íŒŒì‹±
                        if keywords_str.startswith('['):
                            keywords = json.loads(keywords_str)
                        else:
                            keywords = keywords_str.split(',')
                            
                        for kw in keywords:
                            kw = str(kw).strip().strip('"').strip("'")
                            if kw and is_meaningful_token(kw) and is_tech_term(kw):
                                keyword_counter[kw] = keyword_counter.get(kw, 0) + 1
                    except Exception:
                        continue
            
            sorted_keywords = sorted(keyword_counter.items(), key=lambda x: x[1], reverse=True)[:limit]
            result = [{"keyword": k, "count": v} for k, v in sorted_keywords]
            logger.info(f"ğŸ“– {len(result)}ê°œ í‚¤ì›Œë“œ í†µê³„ ë°˜í™˜")
            return result
        
        # DB ë°ì´í„° ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)
        else:
            await ensure_db_initialized()
            
            if ENHANCED_MODULES_AVAILABLE:
                return db.get_keyword_stats(limit)
            else:
                # Fallback implementation
                import sqlite3
                conn = sqlite3.connect("/tmp/news.db")
                cursor = conn.cursor()
                cursor.execute("SELECT keywords FROM articles WHERE keywords IS NOT NULL")
                
                keyword_counter = {}
                for row in cursor.fetchall():
                    try:
                        if row[0]:
                            # Try to parse as JSON, fallback to comma-split
                            try:
                                keywords = json.loads(row[0])
                            except:
                                keywords = row[0].split(',')
                            
                            for kw in keywords:
                                kw = kw.strip()
                                if kw and is_meaningful_token(kw) and is_tech_term(kw):
                                    keyword_counter[kw] = keyword_counter.get(kw, 0) + 1
                    except Exception:
                        continue
                
                conn.close()
                
                sorted_keywords = sorted(keyword_counter.items(), key=lambda x: x[1], reverse=True)[:limit]
                return [{"keyword": k, "count": v} for k, v in sorted_keywords]
            
    except Exception as e:
        logger.error(f"Error getting keyword stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/keywords/network")
async def get_keyword_network(limit: int = Query(30, le=100)):
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT keywords FROM articles WHERE keywords IS NOT NULL")
    
    keyword_docs = []
    for row in cursor.fetchall():
        if row[0]:
            try:
                # JSON í˜•íƒœì˜ í‚¤ì›Œë“œ íŒŒì‹±
                if row[0].startswith('['):
                    keywords = json.loads(row[0])
                else:
                    keywords = row[0].split(',')
                    
                # í•„í„°ë§ëœ í‚¤ì›Œë“œë§Œ ì„ íƒ
                filtered_keywords = []
                for kw in keywords:
                    kw = str(kw).strip().strip('"').strip("'")
                    if kw and is_meaningful_token(kw) and is_tech_term(kw):
                        filtered_keywords.append(kw)
                        
                if filtered_keywords:
                    keyword_docs.append(filtered_keywords)
            except:
                # ê¸°ë³¸ ë¶„í•  ë°©ì‹ìœ¼ë¡œ í´ë°±
                keywords = row[0].split(',')
                filtered_keywords = []
                for kw in keywords:
                    kw = kw.strip()
                    if kw and is_meaningful_token(kw) and is_tech_term(kw):
                        filtered_keywords.append(kw)
                if filtered_keywords:
                    keyword_docs.append(filtered_keywords)
    
    conn.close()
    
    keyword_counter = {}
    cooccurrence = {}
    
    for doc_keywords in keyword_docs:
        for kw in doc_keywords:
            keyword_counter[kw] = keyword_counter.get(kw, 0) + 1
        
        for i, kw1 in enumerate(doc_keywords):
            for kw2 in doc_keywords[i+1:]:
                pair = tuple(sorted([kw1, kw2]))
                cooccurrence[pair] = cooccurrence.get(pair, 0) + 1
    
    top_keywords = sorted(keyword_counter.items(), key=lambda x: x[1], reverse=True)[:limit]
    top_keyword_set = {k for k, _ in top_keywords}
    
    nodes = [{"id": kw, "label": kw, "value": count} for kw, count in top_keywords]
    edges = []
    
    for (kw1, kw2), weight in cooccurrence.items():
        if kw1 in top_keyword_set and kw2 in top_keyword_set and weight > 1:
            edges.append({
                "from": kw1, 
                "to": kw2, 
                "value": weight,
                "label": f"{kw1} â†” {kw2}",
                "title": f"{kw1}ì™€(ê³¼) {kw2}ê°€ í•¨ê»˜ ë‚˜íƒ€ë‚œ íšŸìˆ˜: {weight}íšŒ"
            })
    
    return {"nodes": nodes, "edges": edges}

@app.get("/api/favorites")
async def get_favorites():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT a.* FROM articles a
        JOIN favorites f ON a.id = f.article_id
        ORDER BY f.created_at DESC
    """)
    
    favorites = []
    for row in cursor.fetchall():
        article = dict(row)
        article['is_favorite'] = True
        favorites.append(article)
    
    conn.close()
    return favorites

@app.post("/api/favorites/add")
async def add_favorite(request: FavoriteRequest):
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute(
            "INSERT OR IGNORE INTO favorites (article_id) VALUES (?)",
            (request.article_id,)
        )
        conn.commit()
        return {"success": True, "message": "Favorite added"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    finally:
        conn.close()

@app.delete("/api/favorites/{article_id}")
async def remove_favorite(article_id: int):
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute("DELETE FROM favorites WHERE article_id = ?", (article_id,))
    conn.commit()
    conn.close()
    
    return {"success": True, "message": "Favorite removed"}

@app.get("/api/stats")
async def get_stats(use_json: bool = Query(True, description="Use JSON data as default")):
    """Get general statistics"""
    try:
        # ê¸°ë³¸ì ìœ¼ë¡œ JSON ë°ì´í„° ì‚¬ìš©
        if use_json:
            stats = json_loader.get_stats()
            
            # DBì—ì„œ ì¦ê²¨ì°¾ê¸° ìˆ˜ë§Œ ê°€ì ¸ì™€ì„œ ì¶”ê°€
            try:
                await ensure_db_initialized()
                if ENHANCED_MODULES_AVAILABLE:
                    favorites_result = db.execute_query("SELECT COUNT(*) as count FROM favorites")
                    if favorites_result:
                        stats['total_favorites'] = favorites_result[0]['count']
                else:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM favorites")
                    stats['total_favorites'] = cursor.fetchone()[0]
                    conn.close()
            except Exception as e:
                logger.debug(f"ì¦ê²¨ì°¾ê¸° ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                stats['total_favorites'] = 0
            
            logger.info(f"ğŸ“– JSON í†µê³„ ë°˜í™˜: {stats['total_articles']}ê°œ ê¸°ì‚¬, {stats['total_sources']}ê°œ ì†ŒìŠ¤")
            return stats
        
        # DB ë°ì´í„° ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)
        else:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM articles")
            total_articles = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT source) FROM articles")
            total_sources = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM favorites")
            total_favorites = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT DATE(published) as date, COUNT(*) as count 
                FROM articles 
                WHERE published >= date('now', '-7 days')
                GROUP BY DATE(published)
                ORDER BY date
            """)
            daily_counts = [dict(row) for row in cursor.fetchall()]
            
            conn.close()
            
            return {
                "total_articles": total_articles,
                "total_sources": total_sources,
                "total_favorites": total_favorites,
                "daily_counts": daily_counts
            }
    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        return {
            "total_articles": 0,
            "total_sources": 0, 
            "total_favorites": 0,
            "daily_counts": []
        }

# Inline news collection functions
def collect_from_rss(feed_url: str, source: str, max_items: int = 10):
    """Collect articles from RSS feed"""
    try:
        import feedparser
        import requests
        from datetime import datetime
        
        print(f"ğŸ“¡ Collecting from {source}...")
        
        feed = feedparser.parse(feed_url)
        if not hasattr(feed, 'entries') or not feed.entries:
            return []
        
        articles = []
        for entry in feed.entries[:max_items]:
            try:
                title = getattr(entry, 'title', '').strip()
                link = getattr(entry, 'link', '').strip()
                
                if not title or not link:
                    continue
                
                published = getattr(entry, 'published', datetime.now().strftime('%Y-%m-%d'))
                summary = getattr(entry, 'summary', '')[:500] if hasattr(entry, 'summary') else ''
                
                articles.append({
                    'title': title,
                    'link': link,
                    'published': published,
                    'source': source,
                    'summary': summary
                })
                
            except Exception:
                continue
        
        print(f"âœ… Collected {len(articles)} from {source}")
        return articles
        
    except Exception as e:
        print(f"âŒ Error collecting from {source}: {e}")
        return []

def save_articles_to_db(articles):
    """Save articles to database"""
    if not articles:
        return {'inserted': 0, 'skipped': 0}
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    stats = {'inserted': 0, 'skipped': 0}
    
    for article in articles:
        try:
            cursor.execute("""
                INSERT OR IGNORE INTO articles (title, link, published, source, summary)
                VALUES (?, ?, ?, ?, ?)
            """, (
                article['title'],
                article['link'],
                article['published'],
                article['source'],
                article['summary']
            ))
            
            if cursor.rowcount > 0:
                stats['inserted'] += 1
            else:
                stats['skipped'] += 1
                
        except Exception:
            stats['skipped'] += 1
    
    conn.commit()
    conn.close()
    return stats

def run_collection():
    """Run news collection from major sources"""
    
    # Try simple collector first (no pandas dependency)
    if USE_SIMPLE_COLLECTOR:
        try:
            print("Using simple news collector...")
            # Ensure DB is initialized
            ensure_db_initialized()
            
            # Import and use simple collector with current DB
            import simple_news_collector
            simple_news_collector.DB_PATH = DB_PATH  # Use the same DB path
            
            # Collect news from all feeds
            all_articles = []
            for feed in SIMPLE_FEEDS[:10]:  # Limit to first 10 feeds for quick collection
                articles = collect_from_feed(feed['feed_url'], feed['source'], max_items=5)
                all_articles.extend(articles)
            
            if all_articles:
                # Save to our database
                stats = save_articles_to_db(all_articles)
                return True, len(all_articles), stats
                
        except Exception as e:
            print(f"Error using simple collector: {e}")
    
    # Try full news_collector as second option
    if USE_NEWS_COLLECTOR:
        try:
            print("Using full news_collector...")
            collector_init_db()
            collect_all_news()
            
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM articles")
            total_count = cursor.fetchone()[0]
            conn.close()
            
            return True, total_count, {"message": "Collection completed using news_collector"}
        except Exception as e:
            print(f"Error using news_collector: {e}")
    
    # Fallback to basic RSS collection
    print("Using basic RSS collection...")
    feeds = [
        {"url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„"},
        {"url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìì‹ ë¬¸"},
        {"url": "https://techcrunch.com/feed/", "source": "TechCrunch"},
        {"url": "https://www.theverge.com/rss/index.xml", "source": "The Verge"},
        {"url": "https://www.engadget.com/rss.xml", "source": "Engadget"},
    ]
    
    all_articles = []
    for feed in feeds:
        articles = collect_from_rss(feed["url"], feed["source"])
        all_articles.extend(articles)
    
    if all_articles:
        stats = save_articles_to_db(all_articles)
        return True, len(all_articles), stats
    
    return False, 0, {}

# Enhanced news collection API
@app.post("/api/collect-news")
async def collect_news(background_tasks: BackgroundTasks):
    """Start news collection in background"""
    try:
        await ensure_db_initialized()
        background_tasks.add_task(run_background_collection)
        return {
            "message": "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.", 
            "status": "started",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error starting news collection: {e}")
        return {"message": f"ì˜¤ë¥˜: {str(e)}", "status": "error"}

async def run_background_collection():
    """Background news collection task"""
    try:
        logger.info("ğŸš€ Starting background news collection")
        
        if ENHANCED_MODULES_AVAILABLE:
            result = await collect_news_async(max_feeds=15)  # Limit feeds for background
            logger.info(f"âœ… Background collection completed: {result}")
        else:
            # Fallback collection
            logger.info("Using fallback collector")
            # Implement basic collection here if needed
            
    except Exception as e:
        logger.error(f"âŒ Background collection error: {e}")

@app.post("/api/collect-news-now")
async def collect_news_now(
    max_feeds: Optional[int] = Query(None, description="Maximum number of feeds to process"),
    use_hybrid: bool = Query(True, description="Use hybrid collection (JSON + RSS)")
):
    """í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ (JSON íŒŒì¼ + ìµœê·¼ 1ì£¼ì¼ RSS)"""
    try:
        await ensure_db_initialized()
        
        if ENHANCED_MODULES_AVAILABLE:
            if use_hybrid:
                # NEW: Use hybrid collector
                logger.info("ğŸš€ Starting HYBRID collection (JSON files + recent RSS)")
                logger.info("ğŸ“Š Step 1/3: Initializing hybrid collector...")
                result = await collect_hybrid_data_async()
                
                # Get updated statistics
                try:
                    stats_query = "SELECT COUNT(*) as count FROM articles"
                    sources_query = "SELECT source, COUNT(*) as count FROM articles GROUP BY source ORDER BY count DESC"
                    
                    stats_result = db.execute_query(stats_query)
                    total_articles = stats_result[0]['count'] if stats_result else 0
                    
                    sources_result = db.execute_query(sources_query)
                    by_source = {row['source']: row['count'] for row in sources_result}
                    
                    return {
                        "message": f"í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì™„ë£Œ: JSON {result['json_files']['inserted']}ê°œ + RSS {result['rss_collection']['inserted']}ê°œ = ì´ {result['total_inserted']}ê°œ ì¶”ê°€",
                        "status": "success",
                        "collection_type": "hybrid",
                        "duration": result['duration'],
                        "json_files": result['json_files'],
                        "rss_collection": result['rss_collection'],
                        "total_inserted": result['total_inserted'],
                        "total_processed": result['total_processed'],
                        "total_articles_in_db": total_articles,
                        "by_source": by_source,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as stats_e:
                    logger.warning(f"Error getting stats: {stats_e}")
                    return {
                        "message": f"í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì™„ë£Œ (í†µê³„ ì˜¤ë¥˜): {result['total_inserted']}ê°œ ì¶”ê°€",
                        "status": "success",
                        "collection_result": result
                    }
            else:
                # LEGACY: Weekly RSS only
                logger.info("ğŸš€ Starting weekly news collection (1ì£¼ì¼ ë°ì´í„° only)")
                result = await collect_weekly_news_async()
                
                # Get updated statistics
                try:
                    stats_query = "SELECT COUNT(*) as count FROM articles"
                    sources_query = "SELECT source, COUNT(*) as count FROM articles GROUP BY source ORDER BY count DESC"
                    
                    stats_result = db.execute_query(stats_query)
                    total_articles = stats_result[0]['count'] if stats_result else 0
                    
                    sources_result = db.execute_query(sources_query)
                    by_source = {row['source']: row['count'] for row in sources_result}
                    
                    return {
                        "message": f"1ì£¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {result['stats']['total_inserted']}ê°œ ì‹ ê·œ ê¸°ì‚¬ ì¶”ê°€",
                        "status": "success",
                        "collection_type": "weekly_rss",
                        "collection_period": result['collection_period'],
                        "duration": result['duration'],
                        "processed": result['stats']['total_processed'],
                        "unique_articles": result['stats']['total_unique'],
                        "inserted": result['stats']['total_inserted'],
                        "updated": result['stats']['total_updated'],
                        "skipped": result['stats']['total_skipped'],
                        "total_articles_in_db": total_articles,
                        "by_source": by_source,
                        "successful_feeds": result['successful_feeds'],
                        "failed_feeds": result['failed_feeds'],
                        "total_feeds": result['total_feeds'],
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as stats_e:
                    logger.warning(f"Error getting stats: {stats_e}")
                    return {
                        "message": "1ì£¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ (í†µê³„ ì˜¤ë¥˜)",
                        "status": "success",
                        "collection_result": result
                    }
        else:
            # Fallback simple collection
            if SIMPLE_COLLECTOR_AVAILABLE:
                total_count, stats = collect_all_feeds()
                return {
                    "message": f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {stats.get('inserted', 0)}ê°œ ì‹ ê·œ ì¶”ê°€",
                    "status": "success",
                    "processed": total_count,
                    "inserted": stats.get('inserted', 0),
                    "skipped": stats.get('skipped', 0)
                }
            else:
                raise HTTPException(status_code=500, detail="No news collector available")
            
    except Exception as e:
        logger.error(f"âŒ News collection error: {e}")
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/api/collection-status")
async def get_collection_status():
    """Get current collection status and stats (including hybrid collector info)"""
    try:
        await ensure_db_initialized()
        
        if ENHANCED_MODULES_AVAILABLE:
            # Get database stats
            total_query = "SELECT COUNT(*) as count FROM articles"
            total_articles = db.execute_query(total_query)[0]['count']
            
            recent_query = """
                SELECT COUNT(*) as count FROM articles 
                WHERE created_at > %s
            """ if db.db_type == "postgresql" else """
                SELECT COUNT(*) as count FROM articles 
                WHERE created_at > datetime('now', '-1 day')
            """
            
            if db.db_type == "postgresql":
                params = (datetime.now() - timedelta(days=1),)
                recent_articles = db.execute_query(recent_query, params)[0]['count']
            else:
                recent_articles = db.execute_query(recent_query)[0]['count']
            
            sources_query = "SELECT source, COUNT(*) as count FROM articles GROUP BY source ORDER BY count DESC LIMIT 10"
            top_sources = db.execute_query(sources_query)
            
            # Get hybrid collector info
            try:
                hybrid_info = get_hybrid_collector_info()
            except Exception as e:
                logger.warning(f"Failed to get hybrid collector info: {e}")
                hybrid_info = None
            
            return {
                "status": "active",
                "total_articles": total_articles,
                "recent_articles_24h": recent_articles,
                "top_sources": top_sources,
                "database_type": db.db_type,
                "enhanced_features": True,
                "hybrid_collector": hybrid_info,
                "collection_modes": {
                    "hybrid": "JSON files + recent RSS (default)",
                    "weekly_rss": "RSS only (1 week limit)",
                    "json_only": "Historical JSON files only"
                },
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "status": "basic",
                "enhanced_features": False,
                "message": "ê¸°ë³¸ ìˆ˜ì§‘ ëª¨ë“œ"
            }
            
    except Exception as e:
        logger.error(f"Error getting collection status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì • (React ë¹Œë“œ íŒŒì¼)
frontend_dist = Path(__file__).parent.parent / "frontend" / "news-app" / "dist"
if frontend_dist.exists():
    app.mount("/assets", StaticFiles(directory=str(frontend_dist / "assets")), name="assets")
    
    @app.get("/")
    async def serve_frontend():
        index_path = frontend_dist / "index.html"
        if index_path.exists():
            return FileResponse(str(index_path))
        else:
            return {"message": "Frontend not built. Please run 'npm run build' in frontend/news-app directory"}
else:
    @app.get("/")
    async def root():
        return {"message": "News API Server is running. Frontend not found."}

# ì»¬ë ‰ì…˜ ê´€ë¦¬ API
@app.get("/api/collections")
async def get_collections():
    """ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        ensure_db_initialized()
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Get all collections
        cursor.execute("""
            SELECT c.id, c.name, c.rules, c.created_at, 
                   COUNT(ca.article_id) as article_count
            FROM collections c
            LEFT JOIN collection_articles ca ON c.id = ca.collection_id
            GROUP BY c.id
        """)
        
        collections = []
        for row in cursor.fetchall():
            collection = dict(row)
            collection['rules'] = json.loads(collection['rules']) if collection['rules'] else {}
            collection['count'] = collection['article_count']
            collections.append(collection)
        
        conn.close()
        return collections
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì»¬ë ‰ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/collections")
async def create_collection(request: CollectionRequest):
    """ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        ensure_db_initialized()
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Create the collection
        cursor.execute("""
            INSERT INTO collections (name, rules) VALUES (?, ?)
        """, (request.name, json.dumps(request.rules) if request.rules else None))
        
        collection_id = cursor.lastrowid
        
        # Add articles based on rules
        if request.rules and 'include_keywords' in request.rules:
            keywords = request.rules['include_keywords']
            keyword_filter = ' OR '.join([f"keywords LIKE '%{kw}%'" for kw in keywords])
            
            cursor.execute(f"""
                INSERT INTO collection_articles (collection_id, article_id)
                SELECT ?, id FROM articles 
                WHERE {keyword_filter}
            """, (collection_id,))
            
            added_count = cursor.rowcount
        else:
            added_count = 0
        
        conn.commit()
        conn.close()
        
        return {"message": f"ì»¬ë ‰ì…˜ '{request.name}' ìƒì„± ì™„ë£Œ", "added_articles": added_count, "collection_id": collection_id}
        
    except sqlite3.IntegrityError:
        raise HTTPException(status_code=400, detail=f"ì»¬ë ‰ì…˜ '{request.name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# í‚¤ì›Œë“œ ì¶”ì¶œ API  
@app.post("/api/extract-keywords/{article_id}")
async def extract_article_keywords(article_id: int):
    """íŠ¹ì • ê¸°ì‚¬ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT title, summary FROM articles WHERE id = ?", (article_id,))
        article = cursor.fetchone()
        
        if not article:
            raise HTTPException(status_code=404, detail="ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        text = f"{article['title']} {article['summary'] or ''}"
        keywords = extract_keywords(text)
        
        # í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸
        cursor.execute("UPDATE articles SET keywords = ? WHERE id = ?", 
                      (",".join(keywords), article_id))
        conn.commit()
        conn.close()
        
        return {"keywords": keywords, "message": "í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

# ë²ˆì—­ API
@app.post("/api/translate/{article_id}")  
async def translate_article(article_id: int):
    """íŠ¹ì • ê¸°ì‚¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤."""
    try:
        ensure_db_initialized()
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM articles WHERE id = ?", (article_id,))
        article = cursor.fetchone()
        
        if not article:
            raise HTTPException(status_code=404, detail="ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        article_dict = dict(article)
        
        # Simple translation using basic patterns (without external API)
        # This is a placeholder - in production, use proper translation API
        translated_title = article_dict['title']
        translated_summary = article_dict.get('summary', '')
        
        # Basic keyword-based translation hints
        translation_map = {
            'AI': 'ì¸ê³µì§€ëŠ¥',
            'Machine Learning': 'ë¨¸ì‹ ëŸ¬ë‹',
            'Deep Learning': 'ë”¥ëŸ¬ë‹',
            'Cloud': 'í´ë¼ìš°ë“œ',
            'Security': 'ë³´ì•ˆ',
            'Data': 'ë°ì´í„°',
            'API': 'API',
            'Web': 'ì›¹',
            'Mobile': 'ëª¨ë°”ì¼',
            'Database': 'ë°ì´í„°ë² ì´ìŠ¤'
        }
        
        # Check if article appears to be in English
        is_english = any(word in translated_title.lower() for word in ['the', 'and', 'or', 'is', 'to'])
        
        if is_english:
            # Apply basic translations for known terms
            for eng, kor in translation_map.items():
                if eng.lower() in translated_title.lower():
                    translated_title = f"{translated_title} ({kor} ê´€ë ¨)"
                    break
            
            article_dict['translated_title'] = translated_title
            article_dict['translated_summary'] = f"[ìë™ ë²ˆì—­ ë¯¸ì§€ì›] {translated_summary[:100]}..."
            article_dict['is_translated'] = True
            message = "ê¸°ë³¸ ë²ˆì—­ ì œê³µ (ì „ë¬¸ ë²ˆì—­ ì„œë¹„ìŠ¤ëŠ” API í‚¤ ì„¤ì • í•„ìš”)"
        else:
            article_dict['is_translated'] = False
            message = "í•œêµ­ì–´ ê¸°ì‚¬ì…ë‹ˆë‹¤"
        
        conn.close()
        
        return {"message": message, "article": article_dict}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")

# ====== ìë™ ìš”ì•½ ìƒì„± API ======

@app.post("/api/enhance-summaries")
async def enhance_summaries(
    limit: int = Query(50, description="Number of articles to enhance"),
    force: bool = Query(False, description="Force re-enhancement of all summaries")
):
    """Enhance summaries for articles that lack proper summaries"""
    try:
        await ensure_db_initialized()
        
        if not ENHANCED_MODULES_AVAILABLE:
            raise HTTPException(status_code=503, detail="Auto summarizer not available")
        
        # Get articles that need summary enhancement from JSON data
        if json_loader.load_data():
            articles_data = json_loader.articles_data[:limit] if limit else json_loader.articles_data
            
            enhanced_count = 0
            failed_count = 0
            
            logger.info(f"ğŸ¤– Starting summary enhancement for {len(articles_data)} articles")
            
            for article in articles_data:
                try:
                    original_summary = article.get('summary', '')
                    
                    # Check if summary needs enhancement
                    needs_summary = (
                        not original_summary or 
                        len(original_summary.strip()) < 20 or
                        '[&#8230;]' in original_summary or
                        '...' in original_summary[-10:] or
                        force
                    )
                    
                    if needs_summary:
                        # Generate enhanced summary
                        enhanced_summary = generate_auto_summary(
                            title=article.get('title', ''),
                            url=article.get('link', ''),
                            source=article.get('source', '')
                        )
                        
                        # Update the article data (in memory)
                        article['summary'] = enhanced_summary
                        article['enhanced'] = True
                        enhanced_count += 1
                        
                        logger.debug(f"âœ… Enhanced summary for: {article.get('title', '')[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"âŒ Failed to enhance summary: {e}")
                    failed_count += 1
            
            logger.info(f"ğŸ‰ Summary enhancement completed: {enhanced_count} enhanced, {failed_count} failed")
            
            return {
                "message": f"Summary enhancement completed: {enhanced_count} articles enhanced",
                "enhanced": enhanced_count,
                "failed": failed_count,
                "total": len(articles_data)
            }
        else:
            raise HTTPException(status_code=404, detail="No JSON data loaded")
        
    except Exception as e:
        logger.error(f"âŒ Summary enhancement error: {e}")
        raise HTTPException(status_code=500, detail=f"Summary enhancement failed: {str(e)}")

@app.post("/api/generate-summary")
async def generate_summary(
    title: str,
    url: str = "",
    source: str = ""
):
    """Generate a summary for a given title and URL"""
    try:
        if not ENHANCED_MODULES_AVAILABLE:
            raise HTTPException(status_code=503, detail="Auto summarizer not available")
        
        enhanced_summary = generate_auto_summary(title=title, url=url, source=source)
        
        return {
            "title": title,
            "summary": enhanced_summary,
            "source": source,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"âŒ Summary generation error: {e}")
        raise HTTPException(status_code=500, detail=f"Summary generation failed: {str(e)}")

# ====== ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± API ======

@app.get("/api/wordcloud")
async def generate_wordcloud(
    limit: int = Query(100, description="í‚¤ì›Œë“œ ì œí•œ ìˆ˜"),
    width: int = Query(800, description="ì´ë¯¸ì§€ ë„ˆë¹„"),
    height: int = Query(400, description="ì´ë¯¸ì§€ ë†’ì´")
):
    """íŒŒì´ì¬ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    try:
        # WordCloud ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (í•„ìš”ì‹œ ì„¤ì¹˜)
        try:
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            from matplotlib import font_manager
        except ImportError:
            raise HTTPException(
                status_code=500, 
                detail="WordCloud ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install wordcloud pillow matplotlib ì‹¤í–‰ í•„ìš”"
            )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ìˆ˜ì§‘
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ìµœê·¼ ê¸°ì‚¬ë“¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
        if db.db_type == "postgresql":
            query = """
                SELECT keywords FROM articles 
                WHERE keywords IS NOT NULL 
                AND created_at >= NOW() - INTERVAL '30 days'
                ORDER BY created_at DESC 
                LIMIT %s
            """
            params = (limit * 2,)
        else: # sqlite
            query = """
                SELECT keywords FROM articles 
                WHERE keywords IS NOT NULL 
                AND created_at >= datetime('now', '-30 days')
                ORDER BY created_at DESC 
                LIMIT ?
            """
            params = (limit * 2,)

        cursor.execute(query, params)
        
        results = cursor.fetchall()
        conn.close()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°
        keyword_freq = Counter()
        
        for (keywords_str,) in results:
            if keywords_str:
                if isinstance(keywords_str, str):
                    try:
                        # JSON í˜•íƒœì˜ í‚¤ì›Œë“œ íŒŒì‹±
                        keywords = json.loads(keywords_str) if keywords_str.startswith('[') else keywords_str.split(',')
                    except:
                        keywords = keywords_str.split(',')
                else:
                    keywords = keywords_str
                
                for keyword in keywords:
                    keyword = str(keyword).strip()
                    if keyword and is_meaningful_token(keyword) and is_tech_term(keyword):
                        keyword_freq[keyword] += 1
        
        if not keyword_freq:
            # ê¸°ë³¸ ê¸°ìˆ  í‚¤ì›Œë“œ ì œê³µ (ê¸°ìˆ  ê´€ë ¨ ë‹¨ì–´ë§Œ)
            keyword_freq = Counter({
                'AI': 50, 'ì¸ê³µì§€ëŠ¥': 45, 'ë”¥ëŸ¬ë‹': 35, 'ë¨¸ì‹ ëŸ¬ë‹': 30,
                'ë¸”ë¡ì²´ì¸': 25, 'í´ë¼ìš°ë“œ': 20, 'ë³´ì•ˆ': 18, 'ì†Œí”„íŠ¸ì›¨ì–´': 15,
                'ë°ì´í„°ë² ì´ìŠ¤': 12, 'í”„ë¡œê·¸ë˜ë°': 40, 'ê°œë°œì': 35, 'API': 22, 
                'ë¹…ë°ì´í„°': 28, '5G': 25, 'IoT': 20, 'ë°˜ë„ì²´': 30, 'ì „ê¸°ì°¨': 25
            })
        
        # ìƒìœ„ í‚¤ì›Œë“œë§Œ ì„ íƒí•˜ê³  íŠœí”Œ í˜•íƒœë¡œ ë³€í™˜
        top_keywords_list = list(keyword_freq.most_common(limit))
        
        # ìƒˆë¡œìš´ í•œê¸€ ì§€ì› ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©
        wc_result = render_wordcloud_wc(
            top_keywords_list, 
            auto_korean_font=True, 
            filter_unrenderables=True
        )
        
        if wc_result is None:
            raise HTTPException(status_code=500, detail="ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨")
            
        wordcloud, font_path = wc_result
        
        # í¬ê¸° ì¡°ì •ì„ ìœ„í•´ ë‹¤ì‹œ ì„¤ì •
        wordcloud.width = width
        wordcloud.height = height
        wordcloud.max_words = limit
        wordcloud.relative_scaling = 0.5
        wordcloud.colormap = 'viridis'
        wordcloud.min_font_size = 12
        wordcloud.max_font_size = 80
        wordcloud.prefer_horizontal = 0.7
        
        logger.info(f"âœ… ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ - í°íŠ¸: {font_path or 'ê¸°ë³¸(í•œê¸€ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ)'}")
        
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        img_buffer = io.BytesIO()
        plt.figure(figsize=(width/100, height/100))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.tight_layout(pad=0)
        plt.savefig(img_buffer, format='PNG', bbox_inches='tight', dpi=100)
        plt.close()
        
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.read()).decode('utf-8')
        
        return {
            "wordcloud_image": f"data:image/png;base64,{img_base64}",
            "keyword_count": len(top_keywords_list),
            "top_keywords": [k for k, _ in top_keywords_list[:20]],
            "font_path": font_path,
            "korean_support": font_path is not None
        }
        
    except Exception as e:
        logger.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)